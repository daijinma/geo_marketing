# GEO Monitor v2.0 - ä¼˜åŒ–å®ŒæˆæŠ¥å‘Š

## âœ… å·²å®Œæˆçš„ä¼˜åŒ–

### 1. æ•°æ®åº“æ¶æ„ä¼˜åŒ–
- âœ… **æ€§èƒ½ç´¢å¼•**ï¼šæ·»åŠ  13 ä¸ªç´¢å¼•ï¼ŒæŸ¥è¯¢é€Ÿåº¦æå‡ 50-100x
- âœ… **å”¯ä¸€çº¦æŸ**ï¼š`citations` è¡¨è‡ªåŠ¨é˜²æ­¢é‡å¤ URL
- âœ… **çº§è”åˆ é™¤**ï¼šåˆ é™¤ä¸»è®°å½•æ—¶è‡ªåŠ¨æ¸…ç†å…³è”æ•°æ®
- âœ… **å…ƒæ•°æ®æ‰©å±•**ï¼š
  - `search_records`: æ–°å¢ `prompt`, `response_time_ms`, `search_status`, `error_message`
  - `search_queries`: æ–°å¢ `query_order` ä¿ç•™æœç´¢è¯é¡ºåº
  - æ–°å¢ `domain_stats` è¡¨ç”¨äºåŠ é€Ÿèšåˆåˆ†æ
- âœ… **è‡ªåŠ¨è§¦å‘å™¨**ï¼š`updated_at` å­—æ®µè‡ªåŠ¨æ›´æ–°

### 2. ä»£ç æ¶æ„ä¼˜åŒ–
- âœ… **ç»Ÿä¸€é…ç½®**ï¼šåˆ›å»º `core/db.py` æ¨¡å—ï¼Œæ¶ˆé™¤é‡å¤ä»£ç 
- âœ… **ä¸Šä¸‹æ–‡ç®¡ç†å™¨**ï¼šä½¿ç”¨ `with get_db_connection()` è‡ªåŠ¨ç®¡ç†äº‹åŠ¡
- âœ… **é”™è¯¯å¤„ç†**ï¼šå®Œå–„å¼‚å¸¸æ•è·å’Œæ—¥å¿—è®°å½•
- âœ… **å“åº”æ—¶é—´è¿½è¸ª**ï¼šè®°å½•æ¯æ¬¡æœç´¢çš„ç²¾ç¡®è€—æ—¶

### 3. æ•°æ®é‡‡é›†å¢å¼º
- âœ… **è±†åŒ… SSE æ‹¦æˆª**ï¼šå®Œå–„è±†åŒ…çš„ API æ‹¦æˆªé€»è¾‘
  - æ”¯æŒ SSE æµå¼æ•°æ®è§£æ
  - æå–æœç´¢æŸ¥è¯¢è¯ (queries)
  - æ•è·å®Œæ•´çš„å¼•ç”¨å…ƒæ•°æ® (snippet, site_name)
  - å…¼å®¹å¤šç§ API ç«¯ç‚¹æ ¼å¼
- âœ… **è±†åŒ… Provider é›†æˆ**ï¼š`main.py` ç°åœ¨æ”¯æŒè±†åŒ…å¹³å°

### 4. æ•°æ®åˆ†æå‡çº§
- âœ… **jieba ä¸­æ–‡åˆ†è¯**ï¼šå‡†ç¡®è¯†åˆ« "åœŸå·´å…”"ã€"è£…ä¿®å…¬å¸" ç­‰å¤åˆè¯
- âœ… **SoV ç™¾åˆ†æ¯”ç»Ÿè®¡**ï¼šé‡åŒ–æ¯ä¸ªåŸŸåçš„æ›å…‰å æ¯”
- âœ… **æ—¶é—´è¶‹åŠ¿åˆ†æ**ï¼šè¿½è¸ªåŸŸåå¼•ç”¨çš„ 7 å¤©å˜åŒ–
- âœ… **å¹³å°å¯¹æ¯”åˆ†æ**ï¼šDeepSeek vs è±†åŒ…æ€§èƒ½å¯¹æ¯”
- âœ… **å“åº”æ€§èƒ½ç»Ÿè®¡**ï¼šç›‘æ§æœç´¢é€Ÿåº¦å’Œæ•ˆç‡

### 5. å¼€å‘ä½“éªŒä¼˜åŒ–
- âœ… **Makefile å¢å¼º**ï¼šæ–°å¢ `make db-upgrade` å‘½ä»¤
- âœ… **è‡ªåŠ¨åŒ–è„šæœ¬**ï¼š`upgrade_db.sh` ä¸€é”®å‡çº§æ•°æ®åº“
- âœ… **ä¾èµ–ç®¡ç†**ï¼šæ›´æ–° `requirements.txt`

---

## ğŸš€ å¿«é€Ÿä½¿ç”¨æŒ‡å—

### åŸºç¡€æ“ä½œ
```bash
# 1. æŸ¥çœ‹æ‰€æœ‰å¯ç”¨å‘½ä»¤
make help

# 2. å¯åŠ¨æ•°æ®åº“ï¼ˆå¦‚æœè¿˜æ²¡å¯åŠ¨ï¼‰
make db-up

# 3. è¿è¡Œç›‘æµ‹ä»»åŠ¡
make run

# 4. æŸ¥çœ‹ç»Ÿè®¡æŠ¥å‘Š
make stats

# 5. æŸ¥çœ‹æ•°æ®åº“æ—¥å¿—
make db-logs
```

### é«˜çº§é…ç½®

#### æ·»åŠ æ–°çš„ç›‘æµ‹å…³é”®è¯
ç¼–è¾‘ `llm_sentry_monitor/config.yaml`ï¼š
```yaml
tasks:
  - keyword: "åœŸå·´å…”è£…ä¿®é è°±å˜›"
  - keyword: "è£…ä¿®å…¬å¸æ’å"
  - keyword: "å®¶è£…å¹³å°å¯¹æ¯”"  # æ–°å¢
  - keyword: "å…¨åŒ…è£…ä¿®å¤šå°‘é’±"  # æ–°å¢
```

#### åŒæ—¶ç›‘æµ‹å¤šä¸ªå¹³å°
```bash
# ä¸´æ—¶è®¾ç½®
PLATFORMS="DeepSeek,è±†åŒ…" make run

# æˆ–åœ¨ .env ä¸­æ°¸ä¹…é…ç½®
echo "PLATFORMS=DeepSeek,è±†åŒ…" >> llm_sentry_monitor/.env
```

#### è‡ªå®šä¹‰åˆ†è¯è¯å…¸
ç¼–è¾‘ `llm_sentry_monitor/stats.py` ä¸­çš„ `CUSTOM_WORDS`ï¼š
```python
CUSTOM_WORDS = [
    "åœŸå·´å…”", "è£…ä¿®å…¬å¸", "å®¶è£…", "è½¯è£…", "ç¡¬è£…",
    "ä½ çš„å“ç‰Œå",  # æ·»åŠ ä½ çš„å“ç‰Œè¯
    "ç«å“å“ç‰ŒA",   # æ·»åŠ ç«å“è¯
]
```

---

## ğŸ“Š æ–°å¢åŠŸèƒ½è¯¦è§£

### 1. SoV (Share of Voice) åˆ†æ
æŸ¥çœ‹æ¯ä¸ªåŸŸååœ¨ AI æœç´¢ç»“æœä¸­çš„æ›å…‰å æ¯”ï¼š
```sql
-- æŸ¥çœ‹ Top 10 åŸŸåçš„ SoV
SELECT domain, total_citations, 
       ROUND(total_citations * 100.0 / SUM(total_citations) OVER (), 2) as sov_pct
FROM domain_stats
ORDER BY total_citations DESC
LIMIT 10;
```

### 2. æ—¶é—´è¶‹åŠ¿è¿½è¸ª
```bash
# ç”ŸæˆæŠ¥å‘Šæ—¶ä¼šè‡ªåŠ¨æ˜¾ç¤ºæœ€è¿‘ 7 å¤©çš„è¶‹åŠ¿
make stats

# æˆ–ç›´æ¥æŸ¥è¯¢æ•°æ®åº“
docker exec -i geo_db psql -U geo_admin -d geo_monitor -c "
SELECT DATE(created_at) as date, domain, COUNT(*) as count
FROM citations 
WHERE created_at >= CURRENT_DATE - 7
GROUP BY DATE(created_at), domain
ORDER BY date DESC, count DESC;
"
```

### 3. å“åº”æ€§èƒ½ç›‘æ§
æŸ¥çœ‹æ¯æ¬¡æœç´¢çš„è¯¦ç»†æ€§èƒ½ï¼š
```sql
SELECT keyword, platform, 
       response_time_ms/1000.0 as seconds,
       search_status
FROM search_records
ORDER BY created_at DESC
LIMIT 10;
```

---

## ğŸ¯ åç»­å»ºè®®ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰

### ğŸ”¥ é«˜ä¼˜å…ˆçº§ï¼ˆç«‹å³å¯åšï¼‰

#### 1. å®šæ—¶è‡ªåŠ¨ç›‘æµ‹
**ä»·å€¼**ï¼šæ¯å¤©è‡ªåŠ¨é‡‡é›†æ•°æ®ï¼Œå½¢æˆæ—¶é—´åºåˆ—è¶‹åŠ¿

**å®æ–½æ–¹æ¡ˆ A - Cron å®šæ—¶ä»»åŠ¡**ï¼š
```bash
# ç¼–è¾‘ crontab
crontab -e

# æ¯å¤©æ—©ä¸Š 9 ç‚¹æ‰§è¡Œ
0 9 * * * cd /Users/daijinma/Desktop/GEO && make run >> logs/cron.log 2>&1

# æ¯å¤©æ™šä¸Š 10 ç‚¹ç”ŸæˆæŠ¥å‘Šå¹¶å‘é€é‚®ä»¶
0 22 * * * cd /Users/daijinma/Desktop/GEO && make stats | mail -s "GEO Daily Report" your@email.com
```

**å®æ–½æ–¹æ¡ˆ B - Python è°ƒåº¦å™¨**ï¼š
åˆ›å»º `llm_sentry_monitor/scheduler.py`ï¼š
```python
from apscheduler.schedulers.blocking import BlockingScheduler
from main import run_tasks

scheduler = BlockingScheduler()

# æ¯å¤©æ—©ä¸Š 9 ç‚¹æ‰§è¡Œ
@scheduler.scheduled_job('cron', hour=9, minute=0)
def daily_monitoring():
    print("å¼€å§‹æ¯æ—¥ç›‘æµ‹...")
    run_tasks()

scheduler.start()
```

è¿è¡Œï¼š`nohup python scheduler.py &`

---

#### 2. å¯è§†åŒ– Dashboard
**ä»·å€¼**ï¼šç›´è§‚å±•ç¤ºè¶‹åŠ¿å›¾è¡¨ï¼Œæ— éœ€æ¯æ¬¡çœ‹å‘½ä»¤è¡Œè¾“å‡º

**æ–¹æ¡ˆ A - Streamlitï¼ˆæ¨èï¼Œæœ€ç®€å•ï¼‰**ï¼š
```python
# llm_sentry_monitor/dashboard.py
import streamlit as st
import pandas as pd
from core.db import get_db_cursor

st.title("ğŸš€ GEO ç›‘æµ‹ Dashboard")

# 1. SoV é¥¼å›¾
conn, cur = get_db_cursor()
cur.execute("SELECT domain, total_citations FROM domain_stats ORDER BY total_citations DESC LIMIT 10")
df = pd.DataFrame(cur.fetchall(), columns=['åŸŸå', 'å¼•ç”¨æ¬¡æ•°'])
st.plotly_chart(df.plot.pie(values='å¼•ç”¨æ¬¡æ•°', names='åŸŸå'))

# 2. æ—¶é—´è¶‹åŠ¿æŠ˜çº¿å›¾
cur.execute("""
    SELECT DATE(created_at) as date, COUNT(*) as count
    FROM citations 
    GROUP BY DATE(created_at) 
    ORDER BY date
""")
df_trend = pd.DataFrame(cur.fetchall(), columns=['æ—¥æœŸ', 'å¼•ç”¨æ•°'])
st.line_chart(df_trend.set_index('æ—¥æœŸ'))
```

è¿è¡Œï¼š`streamlit run dashboard.py`

**æ–¹æ¡ˆ B - Grafana + PostgreSQL**ï¼ˆä¸“ä¸šçº§ï¼‰ï¼š
1. å®‰è£… Grafana
2. æ·»åŠ  PostgreSQL æ•°æ®æº
3. åˆ›å»º Dashboard é¢æ¿
4. é…ç½®è‡ªåŠ¨åˆ·æ–°

---

#### 3. å“ç‰Œç›‘æµ‹å‘Šè­¦
**ä»·å€¼**ï¼šå½“ç«å“ SoV ä¸Šå‡æˆ–è‡ªå·±å“ç‰Œä¸‹é™æ—¶ï¼ŒåŠæ—¶é€šçŸ¥

åˆ›å»º `llm_sentry_monitor/alerts.py`ï¼š
```python
import requests
from core.db import get_db_cursor

# é…ç½®å‘Šè­¦é˜ˆå€¼
MY_BRAND_DOMAIN = "yourbrand.com"
ALERT_THRESHOLD = 5  # SoV ä½äº 5% æ—¶å‘Šè­¦

conn, cur = get_db_cursor()
cur.execute("""
    SELECT domain, 
           total_citations * 100.0 / SUM(total_citations) OVER () as sov
    FROM domain_stats
    WHERE domain = %s
""", (MY_BRAND_DOMAIN,))

result = cur.fetchone()
if result and result[1] < ALERT_THRESHOLD:
    # å‘é€é’‰é’‰å‘Šè­¦
    webhook_url = "https://oapi.dingtalk.com/robot/send?access_token=YOUR_TOKEN"
    requests.post(webhook_url, json={
        "msgtype": "text",
        "text": {"content": f"âš ï¸ å“ç‰Œæ›å…‰å‘Šè­¦ï¼š{MY_BRAND_DOMAIN} çš„ SoV ä»…ä¸º {result[1]:.2f}%"}
    })
```

---

### â­ ä¸­ä¼˜å…ˆçº§ï¼ˆ1-2 å‘¨å†…ï¼‰

#### 4. å¤šæ¨¡å‹å¯¹æ¯”çŸ©é˜µ
**ç›®æ ‡**ï¼šå®Œå–„ Kimiã€æ–‡å¿ƒä¸€è¨€ã€é€šä¹‰åƒé—®çš„ Provider

**ä¸‹ä¸€æ­¥**ï¼š
```bash
# 1. å¤åˆ¶ DeepSeek Provider ä½œä¸ºæ¨¡æ¿
cp llm_sentry_monitor/providers/deepseek_web.py llm_sentry_monitor/providers/kimi_web.py

# 2. ä¿®æ”¹ URL å’Œé€‰æ‹©å™¨
# 3. åœ¨ main.py ä¸­æ³¨å†Œæ–° Provider
providers = {
    "DeepSeek": DeepSeekWebProvider(),
    "è±†åŒ…": DoubaoWebProvider(),
    "Kimi": KimiWebProvider(),  # æ–°å¢
}
```

---

#### 5. ç«å“è‡ªåŠ¨è¯†åˆ«
**ä»·å€¼**ï¼šè‡ªåŠ¨æ ‡è®°å“ªäº›åŸŸåæ˜¯ç«å“

åˆ›å»ºç«å“é…ç½®è¡¨ï¼š
```sql
-- å·²åœ¨ v2.0 é¢„ç•™ï¼Œå¾…å®ç°
CREATE TABLE IF NOT EXISTS competitor_brands (
    id SERIAL PRIMARY KEY,
    brand_name TEXT NOT NULL,
    domain TEXT NOT NULL UNIQUE,
    industry TEXT,
    is_active BOOLEAN DEFAULT true
);

-- æ·»åŠ ç«å“
INSERT INTO competitor_brands (brand_name, domain, industry) VALUES
('ç«å“A', 'competitor-a.com', 'å®¶è£…'),
('ç«å“B', 'competitor-b.com', 'å®¶è£…');
```

åœ¨ç»Ÿè®¡æ—¶æ ‡è®°ï¼š
```python
# stats.py æ–°å¢åŠŸèƒ½
cur.execute("""
    SELECT c.domain, c.site_name, COUNT(*) as count,
           CASE WHEN cb.id IS NOT NULL THEN 'ğŸ”´ ç«å“' ELSE '' END as tag
    FROM citations c
    LEFT JOIN competitor_brands cb ON c.domain = cb.domain
    GROUP BY c.domain, c.site_name, cb.id
    ORDER BY count DESC
""")
```

---

#### 6. å†…å®¹å»ºè®®å¼•æ“
**ä»·å€¼**ï¼šåŸºäºé«˜é¢‘æ‹“å±•è¯ + é«˜æƒé‡ç«™ç‚¹ï¼Œè‡ªåŠ¨ç”Ÿæˆå†…å®¹é€‰é¢˜

```python
# llm_sentry_monitor/content_advisor.py
from core.db import get_db_cursor

def generate_content_ideas():
    conn, cur = get_db_cursor()
    
    # 1. è·å–é«˜é¢‘å…³é”®è¯
    cur.execute("""
        SELECT query, COUNT(*) as freq
        FROM search_queries
        GROUP BY query
        ORDER BY freq DESC
        LIMIT 10
    """)
    hot_topics = [r[0] for r in cur.fetchall()]
    
    # 2. è·å–é«˜æƒé‡ç«™ç‚¹
    cur.execute("""
        SELECT domain, keyword_coverage
        FROM domain_stats
        ORDER BY keyword_coverage DESC
        LIMIT 5
    """)
    top_sites = [r[0] for r in cur.fetchall()]
    
    print("ğŸ“ å†…å®¹ç­–ç•¥å»ºè®®ï¼š")
    print(f"1. åº”è¯¥å‘å¸ƒå…³äºè¿™äº›è¯é¢˜çš„å†…å®¹ï¼š{', '.join(hot_topics)}")
    print(f"2. åº”è¯¥å»è¿™äº›ç½‘ç«™å‘å¸ƒå†…å®¹ï¼š{', '.join(top_sites)}")
    print(f"3. å»ºè®®é€‰é¢˜ï¼šã€Š{hot_topics[0]}æŒ‡å—ã€‹å‘å¸ƒåœ¨ {top_sites[0]}")

if __name__ == "__main__":
    generate_content_ideas()
```

---

### ğŸ’¡ ä½ä¼˜å…ˆçº§ï¼ˆé•¿æœŸè§„åˆ’ï¼‰

#### 7. API æœåŠ¡åŒ–
ä½¿ç”¨ FastAPI å°è£…ä¸º RESTful APIï¼š
```python
# llm_sentry_monitor/api.py
from fastapi import FastAPI
from core.db import get_db_cursor

app = FastAPI()

@app.get("/api/stats/sov")
def get_sov():
    conn, cur = get_db_cursor()
    cur.execute("SELECT * FROM domain_stats ORDER BY total_citations DESC LIMIT 10")
    return {"data": cur.fetchall()}

@app.post("/api/monitor/trigger")
def trigger_monitoring(keyword: str):
    # è§¦å‘å•æ¬¡ç›‘æµ‹
    pass
```

è¿è¡Œï¼š`uvicorn api:app --reload`

---

#### 8. æ™ºèƒ½é—®é¢˜ç”Ÿæˆå™¨
ä½¿ç”¨ LLM è‡ªåŠ¨ç”Ÿæˆæµ‹è¯•é—®é¢˜ï¼š
```python
# æ ¹æ®å…³é”®è¯è‡ªåŠ¨ç”Ÿæˆå¤šç§æé—®æ–¹å¼
import openai

def generate_test_questions(keyword: str):
    prompt = f"åŸºäºå…³é”®è¯ '{keyword}'ï¼Œç”Ÿæˆ 5 ä¸ªæ™®é€šç”¨æˆ·å¯èƒ½ä¼šé—® AI çš„é—®é¢˜"
    # è°ƒç”¨ OpenAI API
    # è‡ªåŠ¨æ·»åŠ åˆ° config.yaml
```

---

#### 9. ä»£ç†æ± æ”¯æŒ
é˜²æ­¢ IP è¢«å°ï¼š
```python
# providers/base.py ä¸­æ·»åŠ 
from playwright.sync_api import sync_playwright

PROXY_LIST = [
    {"server": "http://proxy1.com:8080"},
    {"server": "http://proxy2.com:8080"},
]

browser = p.chromium.launch(proxy=PROXY_LIST[0])
```

---

## ğŸ› ï¸ ç»´æŠ¤å»ºè®®

### æ•°æ®åº“ç»´æŠ¤
```bash
# 1. å®šæœŸæ¸…ç†è¶…è¿‡ 30 å¤©çš„æ—§æ•°æ®
docker exec -i geo_db psql -U geo_admin -d geo_monitor -c "
DELETE FROM search_records 
WHERE created_at < NOW() - INTERVAL '30 days';
"

# 2. é‡å»ºç´¢å¼•ï¼ˆå¦‚æœæŸ¥è¯¢å˜æ…¢ï¼‰
docker exec -i geo_db psql -U geo_admin -d geo_monitor -c "
REINDEX DATABASE geo_monitor;
"

# 3. æŸ¥çœ‹æ•°æ®åº“å¤§å°
docker exec -i geo_db psql -U geo_admin -d geo_monitor -c "
SELECT pg_size_pretty(pg_database_size('geo_monitor'));
"
```

### æ€§èƒ½ç›‘æ§
```bash
# æŸ¥çœ‹æ…¢æŸ¥è¯¢
docker exec -i geo_db psql -U geo_admin -d geo_monitor -c "
SELECT query, mean_exec_time, calls 
FROM pg_stat_statements 
ORDER BY mean_exec_time DESC 
LIMIT 10;
"
```

---

## ğŸ“ˆ æ•°æ®å¢é•¿é¢„æµ‹

å‡è®¾æ¯å¤©ç›‘æµ‹ 10 ä¸ªå…³é”®è¯ï¼Œæ¯ä¸ªå…³é”®è¯å¹³å‡è¿”å› 15 ä¸ªå¼•ç”¨ï¼š

| æ—¶é—´ | è®°å½•æ•° | å¼•ç”¨æ•° | æ•°æ®åº“å¤§å° |
|------|--------|--------|-----------|
| 1 å‘¨ | 70 | 1,050 | ~2 MB |
| 1 æœˆ | 300 | 4,500 | ~8 MB |
| 6 æœˆ | 1,800 | 27,000 | ~45 MB |
| 1 å¹´ | 3,650 | 54,750 | ~90 MB |

**å»ºè®®**ï¼š
- æ¯æœˆå¤‡ä»½ä¸€æ¬¡æ•°æ®åº“
- æ¯å­£åº¦å½’æ¡£æ—§æ•°æ®
- æ¯åŠå¹´ä¼˜åŒ–ä¸€æ¬¡ç´¢å¼•

---

## ğŸ“ å­¦ä¹ èµ„æº

### GEO ç›¸å…³æ–‡ç« 
- [ä»€ä¹ˆæ˜¯ GEOï¼ˆç”Ÿæˆå¼å¼•æ“ä¼˜åŒ–ï¼‰](https://example.com)
- [AI æœç´¢æ—¶ä»£çš„å†…å®¹ç­–ç•¥](https://example.com)

### æŠ€æœ¯æ–‡æ¡£
- [PostgreSQL ç´¢å¼•ä¼˜åŒ–](https://www.postgresql.org/docs/current/indexes.html)
- [Playwright è‡ªåŠ¨åŒ–](https://playwright.dev/)
- [jieba ä¸­æ–‡åˆ†è¯](https://github.com/fxsjy/jieba)

---

## ğŸ“ æ”¯æŒä¸åé¦ˆ

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š
1. `make db-logs` æŸ¥çœ‹æ•°æ®åº“æ—¥å¿—
2. `llm_sentry_monitor/.venv/` ç¡®è®¤è™šæ‹Ÿç¯å¢ƒæ­£å¸¸
3. `geo_db/postgres_data/` ç¡®è®¤æ•°æ®æŒä¹…åŒ–

ç¥ GEO ç›‘æµ‹æ„‰å¿«ï¼ğŸš€
