# GEO 实施工程化方案

> **文档目的**：明确 GEO 三大核心策略的工程化实施路径，将理论转化为可执行的技术方案。

---

## 📋 目录

1. [工作方向概览](#1-工作方向概览)
2. [核心策略实施方案](#2-核心策略实施方案)
3. [工程化实施步骤](#3-工程化实施步骤)
4. [技术架构与工具链](#4-技术架构与工具链)
5. [数据收集与处理流程](#5-数据收集与处理流程)
6. [监控与反馈机制](#6-监控与反馈机制)
7. [实施里程碑](#7-实施里程碑)
8. [成功标准](#8-成功标准)
9. [风险与应对](#9-风险与应对)

---

## 1. 工作方向概览

### 1.1 核心目标

基于 GEO 三大支柱战略，建立系统化的内容优化与监测体系：

| 策略维度 | 核心目标 | 工程化指标 |
|---------|---------|-----------|
| **语义足迹扩展** | 覆盖话题集群，提升被 AI 关联概率 | 话题覆盖率、语义相关度评分 |
| **事实密度提升** | 增加统计数据、引用文献和独特见解 | 引用数、数据源质量、信息增益评分 |
| **结构化数据强化** | 优化 Schema.org 标记，提升可解析性 | Schema 覆盖率、数据完整性、解析成功率 |

### 1.2 工作原则

- **系统化**：建立标准化的内容生产与优化流程
- **数据驱动**：基于监测数据（`geo_server`）指导优化方向
- **自动化**：减少人工重复工作，提升效率
- **可迭代**：建立持续改进的反馈循环

---

## 2. 核心策略实施方案

### 2.1 扩大语义足迹 (Expanding Semantic Footprint)

#### 2.1.1 技术方案

**核心思路**：从单一关键词到话题集群，利用查询扇出（Query Fan-out）技术。

**实施工具链**：
```
关键词输入 → 语义扩展引擎 → 话题邻近性映射 → 内容规划矩阵
```

#### 2.1.2 具体实施步骤

**阶段一：语义差距分析（Gap Analysis）**

1. **关键词清单梳理**
   - 输入：核心业务关键词（如"北京装修公司"）
   - 输出：关键词清单 JSON 文件
   - 工具：手动整理或爬虫采集竞品关键词

2. **话题邻近性映射**
   - 方法：利用 LLM（ChatGPT/DeepSeek）生成话题扩展
   - 输入 Prompt 模板：
   ```
   基于核心关键词 "{核心关键词}"，生成至少 10 个语义相关的邻近话题，
   要求覆盖：
   - 用户意图相关问题（如何、为什么、什么时候）
   - 对比性问题（哪个好、有什么区别）
   - 流程性问题（步骤、流程、注意事项）
   - 数据性问题（价格、成本、时间）
   ```

3. **内容规划矩阵生成**
   - 输出格式：Markdown 表格或 CSV
   - 字段：话题名称、用户意图、优先级、现有内容 URL、状态

**阶段二：内容生产与优化**

1. **Hub & Spoke 模型构建**
   - Hub 页面：核心话题主页面（如"北京装修公司"）
   - Spoke 页面：邻近话题子页面（如"北京装修价格"、"北京装修流程"）
   - 内链策略：Hub ↔ Spoke 双向内链

2. **内容模板标准化**
   - 创建内容模板 Markdown 文件
   - 包含：标题、摘要、正文（H2/H3 结构化）、FAQ、相关链接

**阶段三：Prompt 映射（Prompt Mapping）**

1. **反向工程用户提问**
   - 收集 AI 搜索引擎中的真实用户提问
   - 分析问题模式：疑问词 + 关键词 + 意图
   - 建立 Prompt 模式库

2. **针对性优化页面段落**
   - 确保每个页面包含可直接回答常见问题的段落
   - 优化段落结构：问题 → 答案 → 数据支撑 → 引用来源

#### 2.1.3 示例：装修行业话题扩展

```
核心话题：北京装修公司
├─ 邻近话题 1：北京装修价格（2024年最新报价）
│  ├─ 用户意图：了解成本
│  ├─ 关键词扩展：北京装修费用、北京装修预算、北京装修多少钱
│  └─ 内容类型：数据型文章
│
├─ 邻近话题 2：北京装修流程（从设计到验收全流程）
│  ├─ 用户意图：了解流程
│  ├─ 关键词扩展：北京装修步骤、北京装修时间、北京装修周期
│  └─ 内容类型：流程型文章
│
├─ 邻近话题 3：北京装修风格（现代简约、新中式等）
│  ├─ 用户意图：了解选择
│  ├─ 关键词扩展：北京装修风格推荐、北京装修设计、北京装修效果图
│  └─ 内容类型：对比型文章
│
└─ 邻近话题 4：北京装修验收标准（水电、泥工、木工）
   ├─ 用户意图：了解标准
   ├─ 关键词扩展：北京装修验收、北京装修质量、北京装修标准
   └─ 内容类型：规范型文章
```

**输出文件位置**：
- `docs/semantic_footprint/装修行业话题地图.md`
- `docs/semantic_footprint/内容规划矩阵.csv`

---

### 2.2 提高事实密度 (Increasing Fact-Density)

#### 2.2.1 技术方案

**核心思路**：系统化收集统计数据、引用文献和独特见解，建立数据源数据库。

**实施工具链**：
```
数据源采集 → 数据清洗与验证 → 数据存储 → 内容增强
```

#### 2.2.2 数据源分类与采集方案

**1. 统计数据来源**

| 数据源类型 | 数据来源 | 采集方法 | 更新频率 |
|-----------|---------|---------|---------|
| **官方统计** | 国家统计局、住建部 | 手动采集或 API | 季度/年度 |
| **行业报告** | 艾瑞咨询、易观分析、36氪 | RSS/爬虫 + 手动验证 | 季度 |
| **企业数据** | 天眼查、企查查 | API（需授权）或爬虫 | 月度 |
| **用户调研** | 自建问卷、第三方平台 | 自建系统或 API | 月度 |

**2. 引用文献来源**

| 文献类型 | 来源 | 验证方法 |
|---------|------|---------|
| **国家标准** | 国家标准全文公开系统 | 官方文档链接 |
| **行业标准** | 行业标准信息服务平台 | 官方文档链接 |
| **学术论文** | 知网、万方 | DOI 链接 |
| **专家观点** | 行业专家访谈、公开演讲 | 来源标注 |

**3. 独特见解生成方法**

- **数据对比分析**：横向对比（不同城市） + 纵向对比（不同时期）
- **趋势预测**：基于历史数据的线性回归/时间序列分析
- **用户行为洞察**：基于搜索数据、问卷数据的聚类分析

#### 2.2.3 工程化实施步骤

**阶段一：数据源数据库建设**

1. **数据库表结构设计**
   - 表名：`fact_sources`
   - 详细字段见 [数据库扩展方案](#数据库扩展方案)

2. **数据采集脚本开发**
   - 位置：`scripts/data_collection/`
   - 脚本列表：
     - `collect_official_stats.py` - 采集官方统计数据
     - `collect_industry_reports.py` - 采集行业报告
     - `collect_academic_papers.py` - 采集学术论文
     - `validate_sources.py` - 验证数据源有效性

3. **数据清洗与标准化**
   - 统一数据格式（日期、数值、单位）
   - 去重与合并
   - 质量评分（基于来源权威性、时效性）

**阶段二：内容增强工具开发**

1. **数据注入工具**
   - 功能：自动为文章注入相关统计数据
   - 输入：文章 Markdown 文件
   - 输出：增强后的文章（包含数据标注）
   - 实现：基于关键词匹配 + 语义相似度

2. **引用生成工具**
   - 功能：自动生成标准的引用格式
   - 输出格式：
     ```markdown
     > 根据 [国家统计局 2024 年度报告](来源链接) 数据显示，
     > 北京地区 90 平米住宅平均装修成本为 15-25 万元。
     ```

**阶段三：事实密度评分系统**

1. **评分算法**
   - 指标：
     - 统计数据数量（权重：30%）
     - 引用来源数量（权重：30%）
     - 来源权威性（权重：20%）
     - 数据时效性（权重：20%）

2. **评分阈值**
   - 优秀：≥ 80 分
   - 良好：60-79 分
   - 需改进：< 60 分

#### 2.2.4 示例：装修行业数据收集清单

详见 `docs/fact_density/装修行业数据收集清单.yaml`

---

### 2.3 强化结构化数据 (Enhancing Structured Data)

#### 2.3.1 技术方案

**核心思路**：系统化实施 Schema.org 标记，建立实体数据集，提升 AI 解析效率。

**实施工具链**：
```
Schema 规划 → 标记实施 → 验证测试 → 监控优化
```

#### 2.3.2 Schema.org 实施方案

**1. Schema 类型规划**

| Schema 类型 | 适用页面 | 优先级 | 字段要求 |
|------------|---------|--------|---------|
| **Organization** | 网站首页 | 高 | name, url, logo, contactPoint |
| **LocalBusiness** | 企业信息页 | 高 | name, address, telephone, priceRange |
| **Service** | 服务介绍页 | 高 | name, provider, areaServed, description |
| **Product** | 产品页 | 中 | name, description, offers, aggregateRating |
| **Article** | 博客文章 | 中 | headline, author, datePublished, image |
| **FAQPage** | FAQ 页面 | 高 | mainEntity (Question + Answer) |
| **BreadcrumbList** | 所有页面 | 中 | itemListElement |
| **Review** | 评价页面 | 低 | author, reviewRating, reviewBody |

**2. 实施步骤**

**阶段一：Schema 标记生成工具**

1. **工具开发**
   - 位置：`scripts/schema_generator/`
   - 脚本：
     - `generate_organization_schema.py` - 生成 Organization Schema
     - `generate_service_schema.py` - 生成 Service Schema
     - `generate_faq_schema.py` - 生成 FAQ Schema
     - `validate_schema.py` - 验证 Schema 有效性

2. **模板化生成**
   - 输入：YAML 配置文件
   - 输出：JSON-LD 格式的 Schema 标记
   - 示例配置见 `docs/structured_data/templates/organization_schema.yaml`

**阶段二：实体数据集建设**

1. **实体类型定义**
   - 企业实体（Company）
   - 服务实体（Service）
   - 地点实体（Location）
   - 人物实体（Person，专家/设计师）

2. **数据存储**
   - 位置：`data/entities/`
   - 格式：JSON 或 JSON-LD

**阶段三：结构化数据验证与监控**

1. **验证工具**
   - Google 结构化数据测试工具 API
   - 自建验证脚本（基于 Schema.org 规范）

2. **监控指标**
   - Schema 覆盖率（有 Schema 的页面 / 总页面数）
   - Schema 有效性（通过验证的 Schema / 总 Schema 数）
   - 解析成功率（AI 能正确解析的 Schema / 总 Schema 数）

#### 2.3.3 实体数据集来源

**1. 开放数据平台**
- 国家数据：http://data.stats.gov.cn/
- 各地政府开放数据平台
- Kaggle：https://www.kaggle.com/datasets

**2. 知识图谱**
- 百度百科、维基百科（提取实体关系）
- Wikidata：https://www.wikidata.org/
- CN-DBpedia（中文知识图谱）

**3. 商业数据源**
- 天眼查、企查查（企业实体数据）
- 高德地图、百度地图（POI 数据）

**4. 自建数据集**
- 行业数据库（装修公司、材料供应商名录）
- 用户生成数据（UGC）清洗与结构化

#### 2.3.4 示例：结构化数据实施清单

详见 `docs/structured_data/schema实施计划.yaml`

---

### 2.4 持续更新策略

#### 2.4.1 更新频率规划

| 内容类型 | 更新频率 | 触发条件 | 自动化程度 |
|---------|---------|---------|-----------|
| **统计数据** | 按数据发布周期 | 官方数据发布 | 半自动（爬虫 + 人工验证） |
| **价格信息** | 月度 | 市场价格变化 | 半自动 |
| **案例数据** | 季度 | 新案例积累 | 手动 |
| **FAQ 内容** | 按需 | 用户反馈、新问题 | 手动 |
| **Schema 标记** | 按需 | 页面结构变更 | 自动（基于模板） |

#### 2.4.2 基于监控反馈的更新机制

**核心思路**：基于 `geo_server` 的监测数据，智能决定更新优先级。

**更新决策流程**：
```
监测数据 → 分析引用情况 → 识别更新需求 → 生成更新任务 → 执行更新
```

**更新触发条件**：
1. **引用率下降**：某个页面在 AI 回答中的引用频率下降超过 20%
2. **数据过期**：页面中的数据超过有效期（如年度数据已过时）
3. **新话题出现**：监测到新的用户提问，但现有内容未覆盖
4. **竞品超越**：竞品内容被引用的频率超过我方

**实施工具**：
- 位置：`scripts/update_planner/`
- 脚本：
  - `analyze_citation_trends.py` - 分析引用趋势
  - `detect_expired_data.py` - 检测过期数据
  - `generate_update_tasks.py` - 生成更新任务

#### 2.4.3 更新优先级评分

**评分算法**：
- 引用率变化（权重：40%）
- 数据时效性（权重：30%）
- 内容质量评分（权重：20%）
- 竞品对比（权重：10%）

**输出**：更新任务清单（按优先级排序）

---

## 3. 工程化实施步骤

### 3.1 项目结构扩展

在现有 monorepo 结构基础上，增加以下目录：

```
geo_marketing/
├── geo_db/                    # 现有：数据库服务
├── geo_server/                # 现有：后端服务
│
├── geo_content_optimizer/     # 新增：内容优化服务
│   ├── semantic_expander/     # 语义扩展模块
│   │   ├── topic_mapper.py    # 话题映射工具
│   │   ├── prompt_mapper.py   # Prompt 映射工具
│   │   └── content_planner.py # 内容规划工具
│   │
│   ├── fact_enhancer/         # 事实密度提升模块
│   │   ├── data_collector/    # 数据采集
│   │   ├── citation_generator/# 引用生成
│   │   └── quality_scorer.py  # 质量评分
│   │
│   ├── schema_generator/      # 结构化数据生成模块
│   │   ├── schema_builder.py  # Schema 构建器
│   │   ├── entity_extractor.py# 实体提取
│   │   └── validator.py       # 验证工具
│   │
│   └── update_planner/        # 更新规划模块
│       ├── citation_analyzer.py # 引用分析
│       └── task_generator.py  # 任务生成
│
├── data/                      # 新增：数据目录
│   ├── entities/              # 实体数据集
│   ├── fact_sources/          # 事实数据源
│   └── semantic_maps/         # 语义地图
│
├── docs/                      # 新增：文档目录（本目录）
│   ├── semantic_footprint/    # 语义足迹文档
│   ├── fact_density/          # 事实密度文档
│   ├── structured_data/       # 结构化数据文档
│   └── GEO实施工程化方案.md   # 本文档
│
└── scripts/                   # 现有：共享脚本
    ├── data_collection/       # 新增：数据采集脚本
    └── schema_generator/      # 新增：Schema 生成脚本
```

### 3.2 数据库扩展方案

详见 `docs/数据库扩展方案.md`

### 3.3 工具链开发计划

**阶段一：基础工具开发（2-3 周）**
1. 话题映射工具（`topic_mapper.py`）
2. 数据采集脚本框架
3. Schema 生成工具基础版本

**阶段二：自动化流程开发（2-3 周）**
1. 内容规划矩阵生成
2. 事实数据自动注入
3. Schema 批量验证

**阶段三：监控与反馈集成（1-2 周）**
1. 与 `geo_server` 集成
2. 引用趋势分析
3. 自动更新任务生成

---

## 4. 技术架构与工具链

### 4.1 技术栈

| 功能模块 | 技术选型 | 说明 |
|---------|---------|------|
| **语义扩展** | OpenAI API / DeepSeek API | 用于话题邻近性映射 |
| **数据采集** | Scrapy / BeautifulSoup | 网页数据爬取 |
| **数据存储** | PostgreSQL | 结构化数据存储 |
| **Schema 生成** | Python + Jinja2 | 模板化生成 JSON-LD |
| **验证工具** | Google Structured Data API | Schema 验证 |
| **任务调度** | Celery / APScheduler | 定时任务执行 |

### 4.2 工具链集成

```
内容生产流程：
  关键词输入
    ↓
  话题映射工具 → 生成话题清单
    ↓
  内容规划工具 → 生成内容矩阵
    ↓
  数据注入工具 → 增强事实密度
    ↓
  Schema 生成工具 → 添加结构化数据
    ↓
  发布上线

监测反馈流程：
  geo_server → 收集引用数据
    ↓
  引用分析工具 → 识别优化机会
    ↓
  更新任务生成 → 创建更新任务
    ↓
  执行更新 → 循环优化
```

---

## 5. 数据收集与处理流程

### 5.1 数据收集流程

```
确定数据需求
  ↓
选择数据源（官方API / 网页爬取 / 手动采集）
  ↓
数据采集（脚本 / 手动）
  ↓
数据清洗（格式化、去重）
  ↓
数据验证（来源有效性、时效性）
  ↓
存储到数据库
  ↓
质量评分
```

### 5.2 数据处理标准

**数据清洗规则**：
- 统一日期格式（YYYY-MM-DD）
- 统一数值格式（保留小数位、单位标准化）
- 去除 HTML 标签、特殊字符
- 去重与合并

**数据验证规则**：
- 来源链接有效性检查
- 数据完整性检查
- 时效性检查（过期数据标记）

---

## 6. 监控与反馈机制

### 6.1 监控指标

| 指标类型 | 指标名称 | 计算方法 | 目标值 |
|---------|---------|---------|--------|
| **语义覆盖** | 话题覆盖率 | 已覆盖话题数 / 总话题数 | ≥ 80% |
| **事实密度** | 平均事实密度评分 | 所有页面的平均质量评分 | ≥ 70 分 |
| **结构化数据** | Schema 覆盖率 | 有 Schema 的页面数 / 总页面数 | ≥ 90% |
| **引用效果** | 平均引用率 | AI 回答中引用我方内容的频率 | 持续增长 |
| **数据时效** | 数据过期率 | 过期数据数 / 总数据数 | ≤ 10% |

### 6.2 反馈循环

```
每周监控报告生成
  ↓
分析指标变化趋势
  ↓
识别优化机会
  ↓
生成更新任务
  ↓
执行优化
  ↓
下一周监控验证效果
```

**报告内容**：
- 引用趋势分析（哪些页面被引用，哪些下降）
- 话题覆盖缺口（新出现的用户提问未覆盖）
- 数据时效性报告（哪些数据需要更新）
- Schema 验证结果（哪些 Schema 验证失败）

---

## 7. 实施里程碑

### Phase 1: 基础设施搭建（2-3 周）

**目标**：建立基础工具链和数据存储体系

- [ ] 数据库表结构设计并实施
- [ ] 话题映射工具开发（`topic_mapper.py`）
- [ ] 数据采集脚本框架搭建
- [ ] Schema 生成工具基础版本
- [ ] 文档目录结构建立

**交付物**：
- 数据库迁移脚本
- 基础工具代码
- 技术文档

### Phase 2: 内容优化实施（3-4 周）

**目标**：完成首个行业（装修）的完整优化流程

- [ ] 装修行业话题地图生成
- [ ] 内容规划矩阵输出
- [ ] 事实数据源收集（至少 20 个数据源）
- [ ] 核心页面 Schema 标记实施
- [ ] 内容增强（至少 10 个核心页面）

**交付物**：
- 话题地图文档
- 内容规划矩阵
- 增强后的内容页面
- 实施报告

### Phase 3: 自动化流程完善（2-3 周）

**目标**：提升自动化程度，减少人工干预

- [ ] 数据采集自动化（定时任务）
- [ ] 内容质量自动评分系统
- [ ] Schema 批量验证工具
- [ ] 更新任务自动生成

**交付物**：
- 自动化脚本
- 任务调度配置
- 使用文档

### Phase 4: 监控与反馈集成（1-2 周）

**目标**：与现有监控系统集成，建立反馈循环

- [ ] 与 `geo_server` 数据对接
- [ ] 引用趋势分析工具
- [ ] 自动更新任务生成
- [ ] 监控报告自动生成

**交付物**：
- 集成代码
- 监控报告模板
- 使用文档

### Phase 5: 持续优化与扩展（持续）

**目标**：基于数据持续优化，扩展到更多行业

- [ ] 基于监控数据的优化迭代
- [ ] 扩展到其他行业（如有需求）
- [ ] 工具链持续改进
- [ ] 最佳实践沉淀

---

## 8. 成功标准

### 8.1 量化指标

| 指标 | 基线 | 3个月目标 | 6个月目标 |
|-----|------|----------|----------|
| 话题覆盖率 | - | ≥ 60% | ≥ 80% |
| 平均事实密度评分 | - | ≥ 60 分 | ≥ 70 分 |
| Schema 覆盖率 | - | ≥ 70% | ≥ 90% |
| AI 引用率提升 | 基线 | +20% | +50% |
| 数据更新及时率 | - | ≥ 80% | ≥ 90% |

### 8.2 质量指标

- **内容质量**：所有核心页面事实密度评分 ≥ 70 分
- **数据准确性**：数据源验证通过率 ≥ 95%
- **技术规范**：Schema 验证通过率 ≥ 98%
- **流程效率**：自动化覆盖率达到 70% 以上

---

## 9. 风险与应对

| 风险 | 影响 | 应对措施 |
|-----|------|---------|
| **数据源不稳定** | 数据采集中断 | 建立多数据源备份，定期验证 |
| **Schema 规范变化** | 标记失效 | 定期关注 Schema.org 更新，建立版本管理 |
| **AI 算法变化** | 优化策略失效 | 持续监控，快速调整策略 |
| **资源投入不足** | 进度延迟 | 优先级排序，分阶段实施 |
| **数据质量不佳** | 影响优化效果 | 建立严格的数据验证流程 |

---

## 10. 参考资料

- [GEO 原则文档](../why/GEO原则.md)
- [为什么增加博查](../why/为什么增加博查.md)
- [Schema.org 官方文档](https://schema.org/)
- [Google 结构化数据指南](https://developers.google.com/search/docs/appearance/structured-data)
- [Go Fish Digital: GEO Strategy Guide](https://gofishdigital.com)

---

**文档版本**：v1.0  
**最后更新**：2025-01-16  
**维护者**：GEO 项目组

