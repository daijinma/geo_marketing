"""
core/task_executor.py - ä»»åŠ¡æ‰§è¡Œå™¨
å°è£…ä»»åŠ¡æ‰§è¡Œé€»è¾‘ï¼Œæ”¯æŒå¤šå…³é”®è¯ã€å¤šå¹³å°çš„å¼‚æ­¥æ‰§è¡Œ
"""
import os
import time
import logging
import threading
from typing import List, Dict, Any, Optional
from core.db import get_db_connection, update_domain_stats
from core.parser import extract_domain
from providers.deepseek_web import DeepSeekWebProvider
from providers.doubao_web import DoubaoWebProvider
from providers.bocha_api import BochaApiProvider

logger = logging.getLogger(__name__)


def save_to_db(keyword, platform, prompt, result, prompt_type="default", response_time_ms=None, error_message=None, task_id=None, task_query_id=None):
    """
    ä¿å­˜æœç´¢ç»“æœåˆ°æ•°æ®åº“ï¼ˆä» main.py å¤ç”¨ï¼‰
    
    Args:
        keyword: æœç´¢å…³é”®è¯
        platform: å¹³å°åç§°
        prompt: æç¤ºè¯
        result: æœç´¢ç»“æœ
        prompt_type: æç¤ºç±»å‹
        response_time_ms: å“åº”æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
        error_message: é”™è¯¯ä¿¡æ¯
        task_id: task_jobs è¡¨çš„ IDï¼ˆå¯é€‰ï¼Œç”¨äºå…³è”ä»»åŠ¡ï¼‰
        task_query_id: task_query è¡¨çš„ IDï¼ˆå¯é€‰ï¼Œç”¨äºå…³è” executor_sub_query_logï¼‰
    """
    try:
        from providers.doubao_web import ensure_utf8_string
        
        with get_db_connection() as conn:
            cur = conn.cursor()
            
            # ç¡®å®šæœç´¢çŠ¶æ€
            search_status = 'completed' if result and result.get("full_text") else 'failed'
            if error_message:
                search_status = 'failed'
            
            # 1. æ’å…¥æœç´¢è®°å½•ï¼ˆåŒ…å«ä»»åŠ¡å…³è”å­—æ®µï¼‰
            cur.execute("""
                INSERT INTO search_records 
                (keyword, platform, prompt_type, prompt, full_answer, response_time_ms, search_status, error_message, task_id, task_query_id) 
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s) 
                RETURNING id
            """, (
                keyword, 
                platform, 
                prompt_type, 
                prompt,
                result.get("full_text", "") if result else "",
                response_time_ms,
                search_status,
                error_message,
                task_id,
                task_query_id
            ))
            record_id = cur.fetchone()[0]
            
            if not result:
                logger.warning(f"æœç´¢å¤±è´¥ï¼Œä»…ä¿å­˜äº†è®°å½• ID: {record_id}")
                return record_id, 0
            
            # 2. æ’å…¥æ‹“å±•è¯ (å¸¦é¡ºåº)
            for idx, query in enumerate(result.get("queries", []), 1):
                cur.execute(
                    "INSERT INTO search_queries (record_id, query, query_order) VALUES (%s, %s, %s)",
                    (record_id, query, idx)
                )
            
            # 3. æ’å…¥å¼•ç”¨ (åˆ©ç”¨å”¯ä¸€çº¦æŸè‡ªåŠ¨å»é‡)ï¼Œå¹¶ä¿å­˜ citation_id ç”¨äºå…³è”
            citations_count = 0
            citation_ids = {}  # url -> citation_id æ˜ å°„ï¼Œç”¨äºåç»­å…³è” executor_sub_query_log
            for cite in result.get("citations", []):
                url = cite.get("url", "")
                if not url:
                    continue
                    
                domain = extract_domain(url)
                try:
                    # å…ˆå°è¯•æ’å…¥ï¼Œå¦‚æœå†²çªåˆ™æŸ¥è¯¢ç°æœ‰è®°å½•
                    cur.execute("""
                        INSERT INTO citations 
                        (record_id, cite_index, url, domain, title, snippet, site_name) 
                        VALUES (%s, %s, %s, %s, %s, %s, %s)
                        ON CONFLICT (record_id, url) DO NOTHING
                        RETURNING id
                    """, (
                        record_id, 
                        cite.get("cite_index", 0), 
                        url, 
                        domain, 
                        cite.get("title", ""), 
                        cite.get("snippet", ""), 
                        cite.get("site_name", "")
                    ))
                    
                    row = cur.fetchone()
                    if row:
                        # æ–°æ’å…¥çš„è®°å½•
                        citation_id = row[0]
                        citations_count += 1
                        citation_ids[url] = citation_id
                        # æ›´æ–°åŸŸåç»Ÿè®¡
                        update_domain_stats(conn, domain, platform)
                    else:
                        # è®°å½•å·²å­˜åœ¨ï¼ŒæŸ¥è¯¢ citation_id
                        cur.execute("""
                            SELECT id FROM citations 
                            WHERE record_id = %s AND url = %s
                            LIMIT 1
                        """, (record_id, url))
                        row = cur.fetchone()
                        if row:
                            citation_ids[url] = row[0]
                        
                except Exception as e:
                    logger.debug(f"æ’å…¥å¼•ç”¨å¤±è´¥: {e}")
            
            # 4. å¦‚æœæä¾›äº† task_query_idï¼Œä¿å­˜åˆ° executor_sub_query_log è¡¨
            # åˆå¹¶ç­–ç•¥ï¼šå–æ¶ˆ A ç±»å‹ï¼ˆåªæœ‰ sub_queryï¼‰ï¼Œåˆå¹¶åˆ° B ç±»å‹ï¼ˆæœ‰ urlï¼‰
            # æ ¹æ® citation çš„ query_indexes å­—æ®µå»ºç«‹çœŸå®å…³è”
            if task_query_id and result:
                queries = result.get("queries", [])
                citations = result.get("citations", [])
                
                # ä¿å­˜ç½‘å€ä¿¡æ¯ï¼Œæ ¹æ® query_indexes å…³è”å¯¹åº”çš„ query
                # å¦‚æœ citations ä¸ºç©ºä½† queries ä¸ä¸ºç©ºï¼Œåˆ™ä¸ä¿å­˜ï¼ˆå–æ¶ˆ A ç±»å‹ï¼‰
                for cite in citations:
                    url = cite.get("url", "")
                    if not url:
                        continue
                    
                    # è·å–å¯¹åº”çš„ citation_id
                    citation_id = citation_ids.get(url)
                    
                    url = ensure_utf8_string(url) if isinstance(url, str) else url
                    domain = extract_domain(url)
                    title = ensure_utf8_string(cite.get("title", "")) if isinstance(cite.get("title", ""), str) else cite.get("title", "")
                    snippet = ensure_utf8_string(cite.get("snippet", "")) if isinstance(cite.get("snippet", ""), str) else cite.get("snippet", "")
                    site_name = ensure_utf8_string(cite.get("site_name", "")) if isinstance(cite.get("site_name", ""), str) else cite.get("site_name", "")
                    cite_index = cite.get("cite_index", 0)
                    
                    # æ ¹æ® query_indexes è·å–å¯¹åº”çš„ query
                    # query_indexes[0] è¡¨ç¤ºå…³è”åˆ° queries æ•°ç»„çš„ç¬¬å‡ ä¸ª queryï¼ˆç´¢å¼•ä» 0 å¼€å§‹ï¼‰
                    sub_query = None
                    query_indexes = cite.get("query_indexes", [])
                    
                    if queries and query_indexes and len(query_indexes) > 0:
                        # æœ‰æ˜ç¡®çš„ query_indexesï¼ŒæŒ‰ç´¢å¼•å…³è”ï¼ˆDeepSeek ç­‰æƒ…å†µï¼‰
                        query_idx = query_indexes[0]  # åªä½¿ç”¨ç¬¬ä¸€ä¸ªç´¢å¼•
                        # æ£€æŸ¥ç´¢å¼•æœ‰æ•ˆæ€§
                        if isinstance(query_idx, int) and 0 <= query_idx < len(queries):
                            query = queries[query_idx]
                            if query:
                                sub_query = ensure_utf8_string(query) if isinstance(query, str) else query
                    elif queries and len(queries) == 1:
                        # æ²¡æœ‰ query_indexesï¼Œä½†åªæœ‰ä¸€ä¸ª queryï¼ˆè±†åŒ…ç­‰æƒ…å†µï¼‰
                        # è®¤ä¸ºæ‰€æœ‰é“¾æ¥éƒ½å‚è€ƒæ­¤ query
                        query = queries[0]
                        if query:
                            sub_query = ensure_utf8_string(query) if isinstance(query, str) else query
                    # å…¶ä»–æƒ…å†µï¼ˆæ²¡æœ‰ query_indexes ä¸” queries ä¸ä¸º 1 ä¸ªï¼‰ï¼šä¿æŒç°çŠ¶ï¼Œsub_query ä¸º NULL
                    
                    # ä¿å­˜è®°å½•ï¼ˆæ¯ä¸ª citation åªä¿å­˜ä¸€æ¬¡ï¼‰
                    try:
                        cur.execute("""
                            INSERT INTO executor_sub_query_log 
                            (task_query_id, sub_query, record_id, url, domain, title, snippet, site_name, cite_index, citation_id)
                            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                        """, (task_query_id, sub_query, record_id, url, domain, title, snippet, site_name, cite_index, citation_id))
                    except Exception as e:
                        logger.debug(f"æ’å…¥ç½‘å€ä¿¡æ¯å¤±è´¥: {e}")
            
            logger.info(f"âœ… æˆåŠŸä¿å­˜ {platform} çš„æ•°æ®ï¼Œè®°å½• ID: {record_id}")
            logger.info(f"  - æ‹“å±•è¯: {len(result.get('queries', []))} ä¸ª")
            logger.info(f"  - å‚è€ƒç½‘é¡µ: {citations_count} ä¸ª")
            if response_time_ms:
                logger.info(f"  - å“åº”æ—¶é—´: {response_time_ms/1000:.2f} ç§’")
            
            return record_id, citations_count
                
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜åˆ°æ•°æ®åº“å¤±è´¥: {e}", exc_info=True)
        return None, 0


def execute_single_task(keyword: str, platform: str, prompt: str, settings: Dict[str, Any], task_id: Optional[int] = None, task_query_id: Optional[int] = None) -> Dict[str, Any]:
    """
    æ‰§è¡Œå•ä¸ªå…³é”®è¯-å¹³å°ç»„åˆçš„æœç´¢ä»»åŠ¡
    
    Args:
        keyword: æœç´¢å…³é”®è¯
        platform: å¹³å°åç§° (deepseek, doubao, bocha)
        prompt: æç¤ºè¯
        settings: è®¾ç½®å­—å…¸ (headless, timeoutç­‰)
        task_id: task_jobs è¡¨çš„ IDï¼ˆå¯é€‰ï¼Œç”¨äºå…³è”ä»»åŠ¡ï¼‰
        task_query_id: task_query è¡¨çš„ IDï¼ˆå¯é€‰ï¼Œç”¨äºå…³è” executor_sub_query_logï¼‰
    
    Returns:
        åŒ…å«æ‰§è¡Œç»“æœçš„å­—å…¸
    """
    headless = settings.get("headless", False)
    timeout = settings.get("timeout", 60000)
    
    providers = {
        "deepseek": DeepSeekWebProvider(headless=headless, timeout=timeout),
        "doubao": DoubaoWebProvider(headless=headless, timeout=timeout),
        "bocha": BochaApiProvider(headless=headless, timeout=timeout)
    }
    
    # å¹³å°åç§°è§„èŒƒåŒ–
    platform_lower = platform.lower().strip()
    matched_platform = None
    for key in providers.keys():
        if key.lower() == platform_lower:
            matched_platform = key
            break
    
    if not matched_platform:
        return {
            "keyword": keyword,
            "platform": platform,
            "status": "failed",
            "error_message": f"æœªæ‰¾åˆ°å¹³å° [{platform}] çš„ Provider",
            "record_id": None,
            "citations_count": 0
        }
    
    provider = providers[matched_platform]
    logger.info(f"\n{'='*60}")
    logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œä»»åŠ¡: [{keyword}] åœ¨å¹³å° [{matched_platform}]")
    logger.info(f"{'='*60}")
    
    start_time = time.time()
    result = None
    error_message = None
    
    try:
        result = provider.search(keyword, prompt)
        response_time_ms = int((time.time() - start_time) * 1000)
        
        if result and result.get("full_text"):
            record_id, citations_count = save_to_db(
                keyword, matched_platform, prompt, result, 
                prompt_type="api_task", 
                response_time_ms=response_time_ms,
                task_id=task_id,
                task_query_id=task_query_id
            )
            logger.info(f"âœ… {matched_platform} ä»»åŠ¡å®Œæˆ")
            return {
                "keyword": keyword,
                "platform": matched_platform,
                "status": "completed",
                "record_id": record_id,
                "citations_count": citations_count,
                "response_time_ms": response_time_ms
            }
        else:
            error_message = "æœªè¿”å›æœ‰æ•ˆç»“æœ"
            logger.warning(f"âš ï¸ {matched_platform} {error_message}")
            record_id, _ = save_to_db(
                keyword, matched_platform, prompt, None, 
                prompt_type="api_task", 
                error_message=error_message,
                task_id=task_id,
                task_query_id=task_query_id
            )
            return {
                "keyword": keyword,
                "platform": matched_platform,
                "status": "failed",
                "error_message": error_message,
                "record_id": record_id,
                "citations_count": 0
            }
            
    except Exception as e:
        response_time_ms = int((time.time() - start_time) * 1000)
        error_message = str(e)
        logger.error(f"âŒ æ‰§è¡Œä»»åŠ¡å¤±è´¥: {e}", exc_info=True)
        record_id, _ = save_to_db(
            keyword, matched_platform, prompt, None, 
            prompt_type="api_task", 
            response_time_ms=response_time_ms, 
            error_message=error_message,
            task_id=task_id,
            task_query_id=task_query_id
        )
        return {
            "keyword": keyword,
            "platform": matched_platform,
            "status": "failed",
            "error_message": error_message,
            "record_id": record_id,
            "citations_count": 0,
            "response_time_ms": response_time_ms
        }


def execute_task_job(task_id: int, keywords: List[str], platforms: List[str], query_count: int, settings: Dict[str, Any]):
    """
    åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œä»»åŠ¡ä½œä¸š
    
    Args:
        task_id: ä»»åŠ¡ID
        keywords: å…³é”®è¯åˆ—è¡¨
        platforms: å¹³å°åˆ—è¡¨
        query_count: æŸ¥è¯¢æ¬¡æ•°ï¼ˆæ‰§è¡Œè½®æ•°ï¼‰
        settings: è®¾ç½®å­—å…¸
    """
    def run():
        try:
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸º pending
            with get_db_connection() as conn:
                cur = conn.cursor()
                cur.execute(
                    "UPDATE task_jobs SET status = 'pending' WHERE id = %s",
                    (task_id,)
                )
                conn.commit()
            
            # è·å– task_query_id æ˜ å°„ï¼ˆkeyword -> task_query_idï¼‰
            task_query_map = {}
            with get_db_connection() as conn:
                cur = conn.cursor()
                for keyword in keywords:
                    cur.execute("""
                        SELECT id FROM task_query 
                        WHERE task_id = %s AND query = %s
                        LIMIT 1
                    """, (task_id, keyword))
                    row = cur.fetchone()
                    if row:
                        task_query_map[keyword] = row[0]
            
            results = []
            delay = settings.get("delay_between_tasks", 5)
            
            # æŒ‰æ‰§è¡Œæ¬¡æ•°å¾ªç¯ï¼šå¤–å±‚å¾ªç¯æ‰§è¡Œæ¬¡æ•°ï¼Œä¸­å±‚å¾ªç¯æŸ¥è¯¢æ¡ä»¶ï¼Œå†…å±‚å¾ªç¯å¹³å°
            for round_num in range(1, query_count + 1):
                logger.info(f"ğŸ”„ å¼€å§‹ç¬¬ {round_num}/{query_count} è½®æ‰§è¡Œ")
                
                for keyword in keywords:
                    prompt = keyword  # ä½¿ç”¨å…³é”®è¯ä½œä¸ºæç¤ºè¯
                    task_query_id = task_query_map.get(keyword)
                    
                    for platform in platforms:
                        result = execute_single_task(keyword, platform, prompt, settings, task_id, task_query_id)
                        results.append(result)
                        
                        # ä»»åŠ¡é—´å»¶è¿Ÿ
                        if delay > 0:
                            time.sleep(delay)
            
            # æ›´æ–°ä»»åŠ¡ç»“æœ
            import json
            with get_db_connection() as conn:
                cur = conn.cursor()
                cur.execute("""
                    UPDATE task_jobs 
                    SET status = 'done', result_data = %s 
                    WHERE id = %s
                """, (json.dumps(results), task_id))
                conn.commit()
            
            logger.info(f"âœ… ä»»åŠ¡ {task_id} æ‰§è¡Œå®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ ä»»åŠ¡ {task_id} æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸º doneï¼Œä½†è®°å½•é”™è¯¯
            try:
                import json
                with get_db_connection() as conn:
                    cur = conn.cursor()
                    error_result = [{"error": str(e)}]
                    cur.execute("""
                        UPDATE task_jobs 
                        SET status = 'done', result_data = %s 
                        WHERE id = %s
                    """, (json.dumps(error_result), task_id))
                    conn.commit()
            except:
                pass
    
    # åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œ
    thread = threading.Thread(target=run, daemon=True)
    thread.start()

