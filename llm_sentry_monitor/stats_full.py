"""
stats_full.py - GEO æ·±åº¦æ´å¯ŸæŠ¥å‘Šï¼ˆå®Œæ•´ç‰ˆï¼‰
åŠŸèƒ½ï¼šåŒ…å«æ‰€æœ‰åŸºç¡€åˆ†æ + åŸŸåç±»å‹åˆ†å¸ƒã€å¼•ç”¨ä½ç½®åˆ†æã€è·¨å¹³å°ä¸€è‡´æ€§åˆ†æ
"""
import os
import jieba
import jieba.analyse
from datetime import datetime, timedelta
from dotenv import load_dotenv
from tabulate import tabulate
from collections import Counter
from core.db import get_db_cursor
from core.parser import classify_domain_type

load_dotenv()

# è‡ªå®šä¹‰è¯å…¸ï¼ˆè¡Œä¸šæœ¯è¯­ï¼‰
CUSTOM_WORDS = [
    "åœŸå·´å…”", "è£…ä¿®å…¬å¸", "å®¶è£…", "è½¯è£…", "ç¡¬è£…", "å…¨åŒ…", "åŠåŒ…",
    "DeepSeek", "è±†åŒ…", "Kimi", "æ–‡å¿ƒä¸€è¨€", "é€šä¹‰åƒé—®"
]

for word in CUSTOM_WORDS:
    jieba.add_word(word)

# åœç”¨è¯
STOP_WORDS = {
    'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'å¦‚ä½•', 'å“ªäº›', 'ä¸ºä»€ä¹ˆ', 'æ˜¯å¦', 'å¯ä»¥', 'èƒ½å¦',
    '2024', '2025', 'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'å’Œ', 'ä¸', 'æˆ–', 'ç­‰',
    'ä¸€ä¸ª', 'è¿™ä¸ª', 'é‚£ä¸ª', 'è¿›è¡Œ', 'é—®é¢˜', 'ç›¸å…³', 'å…³äº', 'æœ‰å…³'
}

def print_header(title):
    """æ‰“å°æ¼‚äº®çš„æ ‡é¢˜"""
    print("\n" + "="*80)
    print(f"ğŸ“Š {title}")
    print("="*80 + "\n")

def get_date_range_filter(days=None):
    """ç”Ÿæˆæ—¥æœŸè¿‡æ»¤æ¡ä»¶"""
    if days:
        cutoff = datetime.now() - timedelta(days=days)
        return f"WHERE created_at >= '{cutoff.strftime('%Y-%m-%d')}'"
    return ""

def analyze_trust_sources(days=None):
    """1. æ ¸å¿ƒä¿¡ä»»æºåˆ†æï¼ˆå« SoV ç™¾åˆ†æ¯”ï¼‰"""
    print_header("æ ¸å¿ƒä¿¡ä»»æºåˆ†æ - å“ªäº›ç½‘ç«™åœ¨å¤šä¸ªå…³é”®è¯ä¸‹éƒ½è¢« AI ä¿¡ä»»ï¼Ÿ")
    
    conn, cur = get_db_cursor()
    
    date_filter = get_date_range_filter(days)
    
    cur.execute(f"""
        WITH citation_stats AS (
            SELECT 
                domain,
                COUNT(DISTINCT record_id) as keyword_coverage,
                COUNT(*) as total_citations,
                STRING_AGG(DISTINCT site_name, ' | ') as site_names
            FROM citations
            {date_filter}
            GROUP BY domain
        ),
        total AS (
            SELECT SUM(total_citations) as grand_total FROM citation_stats
        )
        SELECT 
            cs.domain,
            cs.keyword_coverage as "è¦†ç›–å…³é”®è¯æ•°",
            cs.total_citations as "æ€»å¼•ç”¨æ¬¡æ•°",
            ROUND(cs.total_citations * 100.0 / t.grand_total, 2) as "SoV(%)",
            cs.site_names as "ç«™ç‚¹åç§°"
        FROM citation_stats cs, total t
        ORDER BY cs.keyword_coverage DESC, cs.total_citations DESC
        LIMIT 15
    """)
    
    rows = cur.fetchall()
    print(tabulate(rows, headers=["åŸŸå", "è¦†ç›–è¯æ•°", "æ€»å¼•ç”¨æ•°", "SoV(%)", "ç«™ç‚¹åç§°"], tablefmt="grid"))
    
    conn.close()

def analyze_search_intent():
    """2. AI æœç´¢æ„å›¾æ´å¯Ÿï¼ˆjieba åˆ†è¯ç‰ˆï¼‰"""
    print_header("AI æœç´¢æ„å›¾æ´å¯Ÿ - AI æœ€å…³æ³¨å“ªäº›æ ¸å¿ƒæ¦‚å¿µï¼Ÿ")
    
    conn, cur = get_db_cursor()
    cur.execute("SELECT query FROM search_queries")
    queries = cur.fetchall()
    
    # ä½¿ç”¨ jieba åˆ†è¯
    all_words = []
    for (query_text,) in queries:
        if not query_text:
            continue
        # ä½¿ç”¨ TF-IDF æå–å…³é”®è¯ï¼ˆæ›´æ™ºèƒ½ï¼‰
        keywords = jieba.analyse.extract_tags(query_text, topK=5, withWeight=False)
        # æˆ–ä½¿ç”¨æ™®é€šåˆ†è¯
        words = jieba.lcut(query_text)
        
        # åˆå¹¶ä¸¤ç§æ–¹å¼çš„ç»“æœ
        all_words.extend([w for w in words + keywords if len(w) > 1 and w not in STOP_WORDS])
    
    word_counts = Counter(all_words).most_common(20)
    
    print(tabulate(word_counts, headers=["æ ¸å¿ƒæ¦‚å¿µ (jiebaåˆ†è¯)", "å‡ºç°é¢‘æ¬¡"], tablefmt="grid"))
    
    conn.close()

def analyze_brand_exposure():
    """3. å“ç‰Œæ›å…‰çŸ©é˜µ"""
    print_header("å“ç‰Œæ›å…‰çŸ©é˜µ - æ¯ä¸ªå…³é”®è¯ä¸‹æ’åå‰ 3 çš„ç«äº‰å¯¹æ‰‹")
    
    conn, cur = get_db_cursor()
    cur.execute("SELECT DISTINCT keyword FROM search_records ORDER BY keyword")
    keywords = [r[0] for r in cur.fetchall()]
    
    matrix_data = []
    for kw in keywords:
        cur.execute("""
            SELECT domain, COUNT(*) as count, STRING_AGG(DISTINCT site_name, ',') as names
            FROM citations c
            JOIN search_records r ON c.record_id = r.id
            WHERE r.keyword = %s
            GROUP BY domain
            ORDER BY count DESC
            LIMIT 3
        """, (kw,))
        
        top_sites = []
        for domain, count, names in cur.fetchall():
            name_display = names.split(',')[0] if names else domain
            top_sites.append(f"{name_display}({count})")
        
        matrix_data.append([kw, " | ".join(top_sites)])
    
    print(tabulate(matrix_data, headers=["ç›‘æ§å…³é”®è¯", "å¤´éƒ¨ç«äº‰åŸŸå (å¼•ç”¨æ¬¡æ•°)"], tablefmt="grid"))
    
    conn.close()

def analyze_time_trends(days=7):
    """4. æ—¶é—´è¶‹åŠ¿åˆ†æ"""
    print_header(f"æ—¶é—´è¶‹åŠ¿åˆ†æ - æœ€è¿‘ {days} å¤©çš„åŸŸåå¼•ç”¨å˜åŒ–")
    
    conn, cur = get_db_cursor()
    
    # è·å– Top 5 åŸŸå
    cur.execute("""
        SELECT domain 
        FROM citations 
        WHERE created_at >= CURRENT_DATE - INTERVAL '%s days'
        GROUP BY domain 
        ORDER BY COUNT(*) DESC 
        LIMIT 5
    """ % days)
    
    top_domains = [r[0] for r in cur.fetchall()]
    
    if not top_domains:
        print("âš ï¸ æš‚æ— æ•°æ®")
        conn.close()
        return
    
    # æŒ‰å¤©ç»Ÿè®¡
    cur.execute("""
        SELECT 
            DATE(created_at) as date,
            domain,
            COUNT(*) as citation_count
        FROM citations
        WHERE domain = ANY(%s) AND created_at >= CURRENT_DATE - INTERVAL '%s days'
        GROUP BY DATE(created_at), domain
        ORDER BY date DESC, citation_count DESC
    """ % ("%s", days), (top_domains,))
    
    rows = cur.fetchall()
    print(tabulate(rows, headers=["æ—¥æœŸ", "åŸŸå", "å¼•ç”¨æ¬¡æ•°"], tablefmt="grid"))
    
    conn.close()

def analyze_platform_comparison():
    """5. å¹³å°å¯¹æ¯”åˆ†æ"""
    print_header("å¹³å°å¯¹æ¯”åˆ†æ - DeepSeek vs è±†åŒ…")
    
    conn, cur = get_db_cursor()
    
    cur.execute("""
        SELECT 
            platform as "å¹³å°",
            COUNT(DISTINCT keyword) as "ç›‘æ§å…³é”®è¯æ•°",
            COUNT(*) as "æ€»æœç´¢æ¬¡æ•°",
            ROUND(AVG(response_time_ms)/1000.0, 2) as "å¹³å‡å“åº”æ—¶é—´(ç§’)",
            SUM(CASE WHEN search_status = 'completed' THEN 1 ELSE 0 END) as "æˆåŠŸæ¬¡æ•°",
            SUM(CASE WHEN search_status = 'failed' THEN 1 ELSE 0 END) as "å¤±è´¥æ¬¡æ•°"
        FROM search_records
        GROUP BY platform
    """)
    
    rows = cur.fetchall()
    print(tabulate(rows, headers=["å¹³å°", "å…³é”®è¯æ•°", "æœç´¢æ¬¡æ•°", "å¹³å‡å“åº”(ç§’)", "æˆåŠŸ", "å¤±è´¥"], tablefmt="grid"))
    
    conn.close()

def analyze_response_performance():
    """6. å“åº”æ€§èƒ½åˆ†æ"""
    print_header("å“åº”æ€§èƒ½åˆ†æ - æœç´¢é€Ÿåº¦ç»Ÿè®¡")
    
    conn, cur = get_db_cursor()
    
    cur.execute("""
        SELECT 
            keyword as "å…³é”®è¯",
            platform as "å¹³å°",
            ROUND(response_time_ms/1000.0, 2) as "å“åº”æ—¶é—´(ç§’)",
            (SELECT COUNT(*) FROM citations WHERE record_id = search_records.id) as "å¼•ç”¨æ•°",
            (SELECT COUNT(*) FROM search_queries WHERE record_id = search_records.id) as "æ‹“å±•è¯æ•°",
            created_at as "æ‰§è¡Œæ—¶é—´"
        FROM search_records
        WHERE search_status = 'completed' AND response_time_ms IS NOT NULL
        ORDER BY created_at DESC
        LIMIT 10
    """)
    
    rows = cur.fetchall()
    print(tabulate(rows, headers=["å…³é”®è¯", "å¹³å°", "å“åº”æ—¶é—´(ç§’)", "å¼•ç”¨æ•°", "æ‹“å±•è¯æ•°", "æ‰§è¡Œæ—¶é—´"], tablefmt="grid"))
    
    conn.close()

def analyze_domain_types(days=None):
    """7. åŸŸåç±»å‹åˆ†å¸ƒåˆ†æ - åˆ†æå¼•ç”¨æ¥æºçš„ç½‘ç«™ç±»å‹åˆ†å¸ƒ"""
    print_header("åŸŸåç±»å‹åˆ†å¸ƒåˆ†æ - AI å¯¹ä¸åŒç±»å‹ç½‘ç«™çš„åå¥½")
    
    conn, cur = get_db_cursor()
    
    date_filter = get_date_range_filter(days)
    
    # è·å–æ‰€æœ‰å¼•ç”¨åŠå…¶ URL
    if date_filter:
        cur.execute(f"""
            SELECT url, COUNT(*) as citation_count
            FROM citations
            {date_filter}
            GROUP BY url
        """)
    else:
        cur.execute("""
            SELECT url, COUNT(*) as citation_count
            FROM citations
            GROUP BY url
        """)
    
    citations_data = cur.fetchall()
    
    # åˆ†ç±»ç»Ÿè®¡
    type_stats = Counter()
    type_citation_counts = Counter()
    
    for url, count in citations_data:
        domain_type = classify_domain_type(url)
        type_stats[domain_type] += 1
        type_citation_counts[domain_type] += count
    
    # è®¡ç®—æ€»æ•°å’Œå æ¯”
    total_types = sum(type_stats.values())
    total_citations = sum(type_citation_counts.values())
    
    if total_types == 0:
        print("âš ï¸ æš‚æ— æ•°æ®")
        conn.close()
        return
    
    # å‡†å¤‡è¡¨æ ¼æ•°æ®
    table_data = []
    for domain_type in ['å®˜ç½‘', 'çŸ¥ä¹', 'è‡ªåª’ä½“', 'æ–°é—»ç«™', 'è®ºå›', 'å…¶ä»–']:
        if domain_type in type_stats:
            type_count = type_stats[domain_type]
            citation_count = type_citation_counts[domain_type]
            type_percentage = round(type_count * 100.0 / total_types, 2)
            citation_percentage = round(citation_count * 100.0 / total_citations, 2) if total_citations > 0 else 0
            table_data.append([domain_type, type_count, type_percentage, citation_count, citation_percentage])
    
    print(tabulate(table_data, headers=["ç½‘ç«™ç±»å‹", "åŸŸåæ•°é‡", "åŸŸåå æ¯”(%)", "å¼•ç”¨æ¬¡æ•°", "å¼•ç”¨å æ¯”(%)"], tablefmt="grid"))
    
    conn.close()

def analyze_citation_positions():
    """8. å¼•ç”¨ä½ç½®åˆ†æ - åˆ†æå¼•ç”¨åœ¨å›ç­”ä¸­çš„ä½ç½®åˆ†å¸ƒ"""
    print_header("å¼•ç”¨ä½ç½®åˆ†æ - å“ªäº›åŸŸåæ›´å¸¸å‡ºç°åœ¨å›ç­”çš„å¼€å¤´/ç»“å°¾ï¼Ÿ")
    
    conn, cur = get_db_cursor()
    
    # è·å–æ¯ä¸ªè®°å½•çš„å¼•ç”¨æ€»æ•°å’Œä½ç½®ä¿¡æ¯
    cur.execute("""
        SELECT 
            c.record_id,
            c.cite_index,
            c.domain,
            c.url,
            (SELECT COUNT(*) FROM citations WHERE record_id = c.record_id) as total_citations
        FROM citations c
        ORDER BY c.record_id, c.cite_index
    """)
    
    citations = cur.fetchall()
    
    # åˆ†ç±»ç»Ÿè®¡
    position_stats = {
        'å¼€å¤´': Counter(),  # cite_index <= 3
        'ä¸­é—´': Counter(),  # 3 < cite_index <= (total - 3)
        'ç»“å°¾': Counter()   # cite_index > (total - 3)
    }
    
    # æŒ‰ record_id åˆ†ç»„å¤„ç†
    current_record_id = None
    record_citations = []
    
    for record_id, cite_index, domain, url, total_citations in citations:
        if current_record_id != record_id:
            # å¤„ç†ä¸Šä¸€ä¸ªè®°å½•
            if current_record_id is not None and record_citations:
                for rec_cite_index, rec_domain, rec_total in record_citations:
                    if rec_total <= 6:
                        # å¦‚æœæ€»å¼•ç”¨æ•° <= 6ï¼Œå…¨éƒ¨ç®—ä½œä¸­é—´
                        position_stats['ä¸­é—´'][rec_domain] += 1
                    else:
                        if rec_cite_index <= 3:
                            position_stats['å¼€å¤´'][rec_domain] += 1
                        elif rec_cite_index > (rec_total - 3):
                            position_stats['ç»“å°¾'][rec_domain] += 1
                        else:
                            position_stats['ä¸­é—´'][rec_domain] += 1
            
            # å¼€å§‹æ–°è®°å½•
            current_record_id = record_id
            record_citations = []
        
        record_citations.append((cite_index, domain, total_citations))
    
    # å¤„ç†æœ€åä¸€ä¸ªè®°å½•
    if record_citations:
        for rec_cite_index, rec_domain, rec_total in record_citations:
            if rec_total <= 6:
                position_stats['ä¸­é—´'][rec_domain] += 1
            else:
                if rec_cite_index <= 3:
                    position_stats['å¼€å¤´'][rec_domain] += 1
                elif rec_cite_index > (rec_total - 3):
                    position_stats['ç»“å°¾'][rec_domain] += 1
                else:
                    position_stats['ä¸­é—´'][rec_domain] += 1
    
    # ç»Ÿè®¡æ¯ä¸ªä½ç½®çš„å‰10ä¸ªåŸŸå
    print("\nğŸ“ å¼€å¤´ä½ç½®ï¼ˆå‰3ä¸ªå¼•ç”¨ï¼‰Top 10 åŸŸåï¼š")
    top_start = position_stats['å¼€å¤´'].most_common(10)
    if top_start:
        print(tabulate(top_start, headers=["åŸŸå", "å‡ºç°æ¬¡æ•°"], tablefmt="grid"))
    else:
        print("  æš‚æ— æ•°æ®")
    
    print("\nğŸ“ ä¸­é—´ä½ç½® Top 10 åŸŸåï¼š")
    top_middle = position_stats['ä¸­é—´'].most_common(10)
    if top_middle:
        print(tabulate(top_middle, headers=["åŸŸå", "å‡ºç°æ¬¡æ•°"], tablefmt="grid"))
    else:
        print("  æš‚æ— æ•°æ®")
    
    print("\nğŸ“ ç»“å°¾ä½ç½®ï¼ˆå3ä¸ªå¼•ç”¨ï¼‰Top 10 åŸŸåï¼š")
    top_end = position_stats['ç»“å°¾'].most_common(10)
    if top_end:
        print(tabulate(top_end, headers=["åŸŸå", "å‡ºç°æ¬¡æ•°"], tablefmt="grid"))
    else:
        print("  æš‚æ— æ•°æ®")
    
    # æ±‡æ€»ç»Ÿè®¡
    print("\nğŸ“Š ä½ç½®åˆ†å¸ƒæ±‡æ€»ï¼š")
    summary_data = [
        ['å¼€å¤´', sum(position_stats['å¼€å¤´'].values())],
        ['ä¸­é—´', sum(position_stats['ä¸­é—´'].values())],
        ['ç»“å°¾', sum(position_stats['ç»“å°¾'].values())]
    ]
    total_positions = sum([sum(c.values()) for c in position_stats.values()])
    if total_positions > 0:
        for row in summary_data:
            row.append(round(row[1] * 100.0 / total_positions, 2))
        print(tabulate(summary_data, headers=["ä½ç½®", "å¼•ç”¨æ•°", "å æ¯”(%)"], tablefmt="grid"))
    
    conn.close()

def analyze_cross_platform_consistency():
    """9. è·¨å¹³å°ä¸€è‡´æ€§åˆ†æ - å¯¹æ¯”åŒä¸€å…³é”®è¯åœ¨ä¸åŒå¹³å°çš„å¼•ç”¨å·®å¼‚"""
    print_header("è·¨å¹³å°ä¸€è‡´æ€§åˆ†æ - DeepSeek vs è±†åŒ…çš„å¼•ç”¨å·®å¼‚")
    
    conn, cur = get_db_cursor()
    
    # è·å–æ‰€æœ‰å…³é”®è¯
    cur.execute("SELECT DISTINCT keyword FROM search_records ORDER BY keyword")
    keywords = [r[0] for r in cur.fetchall()]
    
    if not keywords:
        print("âš ï¸ æš‚æ— æ•°æ®")
        conn.close()
        return
    
    # å¯¹æ¯ä¸ªå…³é”®è¯è¿›è¡Œè·¨å¹³å°åˆ†æ
    comparison_data = []
    
    for keyword in keywords:
        # è·å–è¯¥å…³é”®è¯åœ¨ä¸åŒå¹³å°çš„å¼•ç”¨åŸŸå
        cur.execute("""
            SELECT 
                r.platform,
                c.domain
            FROM search_records r
            JOIN citations c ON r.id = c.record_id
            WHERE r.keyword = %s
            GROUP BY r.platform, c.domain
        """, (keyword,))
        
        platform_domains = {}
        for platform, domain in cur.fetchall():
            if platform not in platform_domains:
                platform_domains[platform] = set()
            platform_domains[platform].add(domain)
        
        if len(platform_domains) < 2:
            # åªæœ‰ä¸€ä¸ªå¹³å°çš„æ•°æ®ï¼Œè·³è¿‡
            continue
        
        # è®¡ç®—é‡å åŸŸå
        platforms = list(platform_domains.keys())
        if len(platforms) >= 2:
            common_domains = platform_domains[platforms[0]] & platform_domains[platforms[1]]
            all_domains = platform_domains[platforms[0]] | platform_domains[platforms[1]]
            
            overlap_count = len(common_domains)
            total_unique = len(all_domains)
            overlap_rate = round(overlap_count * 100.0 / total_unique, 2) if total_unique > 0 else 0
            
            # å¹³å°ç‰¹æœ‰åŸŸå
            platform1_unique = platform_domains[platforms[0]] - platform_domains[platforms[1]]
            platform2_unique = platform_domains[platforms[1]] - platform_domains[platforms[0]]
            
            comparison_data.append({
                'keyword': keyword,
                'platform1': platforms[0],
                'platform2': platforms[1],
                'platform1_domains': len(platform_domains[platforms[0]]),
                'platform2_domains': len(platform_domains[platforms[1]]),
                'common_domains': overlap_count,
                'overlap_rate': overlap_rate,
                'platform1_unique': len(platform1_unique),
                'platform2_unique': len(platform2_unique)
            })
    
    if not comparison_data:
        print("âš ï¸ æš‚æ— è·¨å¹³å°å¯¹æ¯”æ•°æ®ï¼ˆéœ€è¦è‡³å°‘ä¸¤ä¸ªå¹³å°çš„æ•°æ®ï¼‰")
        conn.close()
        return
    
    # æ˜¾ç¤ºå¯¹æ¯”è¡¨æ ¼
    table_data = []
    for comp in comparison_data:
        table_data.append([
            comp['keyword'],
            comp['platform1'],
            comp['platform1_domains'],
            comp['platform2'],
            comp['platform2_domains'],
            comp['common_domains'],
            f"{comp['overlap_rate']}%",
            comp['platform1_unique'],
            comp['platform2_unique']
        ])
    
    print(tabulate(table_data, headers=[
        "å…³é”®è¯", 
        "å¹³å°1", "å¹³å°1åŸŸåæ•°",
        "å¹³å°2", "å¹³å°2åŸŸåæ•°",
        "å…±åŒåŸŸå", "é‡å ç‡",
        "å¹³å°1ç‰¹æœ‰", "å¹³å°2ç‰¹æœ‰"
    ], tablefmt="grid"))
    
    # è®¡ç®—å¹³å‡é‡å ç‡
    if comparison_data:
        avg_overlap = sum(c['overlap_rate'] for c in comparison_data) / len(comparison_data)
        print(f"\nğŸ“ˆ å¹³å‡é‡å ç‡: {round(avg_overlap, 2)}%")
    
    conn.close()

def main():
    """ä¸»å‡½æ•° - å®Œæ•´ç‰ˆåˆ†æ"""
    print("\n" + "ğŸš€ "*20)
    print("    GEO æ·±åº¦æ´å¯ŸæŠ¥å‘Š (å®Œæ•´ç‰ˆ v3.0)")
    print("    é›†æˆï¼šåŸºç¡€åˆ†æ + åŸŸåç±»å‹åˆ†å¸ƒ + å¼•ç”¨ä½ç½®åˆ†æ + è·¨å¹³å°ä¸€è‡´æ€§")
    print("ğŸš€ "*20)
    
    try:
        # åŸºç¡€åˆ†æï¼ˆ6ä¸ªï¼‰
        # 1. æ ¸å¿ƒä¿¡ä»»æºï¼ˆè¿‘ 7 å¤©ï¼‰
        analyze_trust_sources(days=7)
        
        # 2. AI æœç´¢æ„å›¾ï¼ˆjieba åˆ†è¯ï¼‰
        analyze_search_intent()
        
        # 3. å“ç‰Œæ›å…‰çŸ©é˜µ
        analyze_brand_exposure()
        
        # 4. æ—¶é—´è¶‹åŠ¿åˆ†æ
        analyze_time_trends(days=7)
        
        # 5. å¹³å°å¯¹æ¯”åˆ†æ
        analyze_platform_comparison()
        
        # 6. å“åº”æ€§èƒ½åˆ†æ
        analyze_response_performance()
        
        # æ–°å¢åˆ†æï¼ˆ3ä¸ªï¼‰
        # 7. åŸŸåç±»å‹åˆ†å¸ƒåˆ†æ
        analyze_domain_types(days=7)
        
        # 8. å¼•ç”¨ä½ç½®åˆ†æ
        analyze_citation_positions()
        
        # 9. è·¨å¹³å°ä¸€è‡´æ€§åˆ†æ
        analyze_cross_platform_consistency()
        
        print("\n" + "="*80)
        print("âœ… å®Œæ•´ç‰ˆæŠ¥å‘Šç”Ÿæˆå®Œæˆï¼")
        print("="*80 + "\n")
        
    except Exception as e:
        print(f"\nâŒ è·å–ç»Ÿè®¡æ•°æ®å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

