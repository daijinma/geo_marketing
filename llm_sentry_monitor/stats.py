"""
stats.py - GEO æ·±åº¦æ´å¯ŸæŠ¥å‘Šï¼ˆå¢å¼ºç‰ˆï¼‰
åŠŸèƒ½ï¼šjieba åˆ†è¯ã€SoV ç™¾åˆ†æ¯”ã€æ—¶é—´è¶‹åŠ¿åˆ†æ
"""
import os
import jieba
import jieba.analyse
from datetime import datetime, timedelta
from dotenv import load_dotenv
from tabulate import tabulate
from collections import Counter
from core.db import get_db_cursor

load_dotenv()

# è‡ªå®šä¹‰è¯å…¸ï¼ˆè¡Œä¸šæœ¯è¯­ï¼‰
CUSTOM_WORDS = [
    "åœŸå·´å…”", "è£…ä¿®å…¬å¸", "å®¶è£…", "è½¯è£…", "ç¡¬è£…", "å…¨åŒ…", "åŠåŒ…",
    "DeepSeek", "è±†åŒ…", "Kimi", "æ–‡å¿ƒä¸€è¨€", "é€šä¹‰åƒé—®"
]

for word in CUSTOM_WORDS:
    jieba.add_word(word)

# åœç”¨è¯
STOP_WORDS = {
    'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'å¦‚ä½•', 'å“ªäº›', 'ä¸ºä»€ä¹ˆ', 'æ˜¯å¦', 'å¯ä»¥', 'èƒ½å¦',
    '2024', '2025', 'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'å’Œ', 'ä¸', 'æˆ–', 'ç­‰',
    'ä¸€ä¸ª', 'è¿™ä¸ª', 'é‚£ä¸ª', 'è¿›è¡Œ', 'é—®é¢˜', 'ç›¸å…³', 'å…³äº', 'æœ‰å…³'
}

def print_header(title):
    """æ‰“å°æ¼‚äº®çš„æ ‡é¢˜"""
    print("\n" + "="*80)
    print(f"ğŸ“Š {title}")
    print("="*80 + "\n")

def get_date_range_filter(days=None):
    """ç”Ÿæˆæ—¥æœŸè¿‡æ»¤æ¡ä»¶"""
    if days:
        cutoff = datetime.now() - timedelta(days=days)
        return f"WHERE created_at >= '{cutoff.strftime('%Y-%m-%d')}'"
    return ""

def analyze_trust_sources(days=None):
    """1. æ ¸å¿ƒä¿¡ä»»æºåˆ†æï¼ˆå« SoV ç™¾åˆ†æ¯”ï¼‰"""
    print_header("æ ¸å¿ƒä¿¡ä»»æºåˆ†æ - å“ªäº›ç½‘ç«™åœ¨å¤šä¸ªå…³é”®è¯ä¸‹éƒ½è¢« AI ä¿¡ä»»ï¼Ÿ")
    
    conn, cur = get_db_cursor()
    
    date_filter = get_date_range_filter(days)
    
    cur.execute(f"""
        WITH citation_stats AS (
            SELECT 
                domain,
                COUNT(DISTINCT record_id) as keyword_coverage,
                COUNT(*) as total_citations,
                STRING_AGG(DISTINCT site_name, ' | ') as site_names
            FROM citations
            {date_filter}
            GROUP BY domain
        ),
        total AS (
            SELECT SUM(total_citations) as grand_total FROM citation_stats
        )
        SELECT 
            cs.domain,
            cs.keyword_coverage as "è¦†ç›–å…³é”®è¯æ•°",
            cs.total_citations as "æ€»å¼•ç”¨æ¬¡æ•°",
            ROUND(cs.total_citations * 100.0 / t.grand_total, 2) as "SoV(%)",
            cs.site_names as "ç«™ç‚¹åç§°"
        FROM citation_stats cs, total t
        ORDER BY cs.keyword_coverage DESC, cs.total_citations DESC
        LIMIT 15
    """)
    
    rows = cur.fetchall()
    print(tabulate(rows, headers=["åŸŸå", "è¦†ç›–è¯æ•°", "æ€»å¼•ç”¨æ•°", "SoV(%)", "ç«™ç‚¹åç§°"], tablefmt="grid"))
    
    conn.close()

def analyze_search_intent():
    """2. AI æœç´¢æ„å›¾æ´å¯Ÿï¼ˆjieba åˆ†è¯ç‰ˆï¼‰"""
    print_header("AI æœç´¢æ„å›¾æ´å¯Ÿ - AI æœ€å…³æ³¨å“ªäº›æ ¸å¿ƒæ¦‚å¿µï¼Ÿ")
    
    conn, cur = get_db_cursor()
    cur.execute("SELECT query FROM search_queries")
    queries = cur.fetchall()
    
    # ä½¿ç”¨ jieba åˆ†è¯
    all_words = []
    for (query_text,) in queries:
        if not query_text:
            continue
        # ä½¿ç”¨ TF-IDF æå–å…³é”®è¯ï¼ˆæ›´æ™ºèƒ½ï¼‰
        keywords = jieba.analyse.extract_tags(query_text, topK=5, withWeight=False)
        # æˆ–ä½¿ç”¨æ™®é€šåˆ†è¯
        words = jieba.lcut(query_text)
        
        # åˆå¹¶ä¸¤ç§æ–¹å¼çš„ç»“æœ
        all_words.extend([w for w in words + keywords if len(w) > 1 and w not in STOP_WORDS])
    
    word_counts = Counter(all_words).most_common(20)
    
    print(tabulate(word_counts, headers=["æ ¸å¿ƒæ¦‚å¿µ (jiebaåˆ†è¯)", "å‡ºç°é¢‘æ¬¡"], tablefmt="grid"))
    
    conn.close()

def analyze_brand_exposure():
    """3. å“ç‰Œæ›å…‰çŸ©é˜µ"""
    print_header("å“ç‰Œæ›å…‰çŸ©é˜µ - æ¯ä¸ªå…³é”®è¯ä¸‹æ’åå‰ 3 çš„ç«äº‰å¯¹æ‰‹")
    
    conn, cur = get_db_cursor()
    cur.execute("SELECT DISTINCT keyword FROM search_records ORDER BY keyword")
    keywords = [r[0] for r in cur.fetchall()]
    
    matrix_data = []
    for kw in keywords:
        cur.execute("""
            SELECT domain, COUNT(*) as count, STRING_AGG(DISTINCT site_name, ',') as names
            FROM citations c
            JOIN search_records r ON c.record_id = r.id
            WHERE r.keyword = %s
            GROUP BY domain
            ORDER BY count DESC
            LIMIT 3
        """, (kw,))
        
        top_sites = []
        for domain, count, names in cur.fetchall():
            name_display = names.split(',')[0] if names else domain
            top_sites.append(f"{name_display}({count})")
        
        matrix_data.append([kw, " | ".join(top_sites)])
    
    print(tabulate(matrix_data, headers=["ç›‘æ§å…³é”®è¯", "å¤´éƒ¨ç«äº‰åŸŸå (å¼•ç”¨æ¬¡æ•°)"], tablefmt="grid"))
    
    conn.close()

def analyze_time_trends(days=7):
    """4. æ—¶é—´è¶‹åŠ¿åˆ†æï¼ˆæ–°å¢ï¼‰"""
    print_header(f"æ—¶é—´è¶‹åŠ¿åˆ†æ - æœ€è¿‘ {days} å¤©çš„åŸŸåå¼•ç”¨å˜åŒ–")
    
    conn, cur = get_db_cursor()
    
    # è·å– Top 5 åŸŸå
    cur.execute("""
        SELECT domain 
        FROM citations 
        WHERE created_at >= CURRENT_DATE - INTERVAL '%s days'
        GROUP BY domain 
        ORDER BY COUNT(*) DESC 
        LIMIT 5
    """ % days)
    
    top_domains = [r[0] for r in cur.fetchall()]
    
    if not top_domains:
        print("âš ï¸ æš‚æ— æ•°æ®")
        conn.close()
        return
    
    # æŒ‰å¤©ç»Ÿè®¡
    cur.execute("""
        SELECT 
            DATE(created_at) as date,
            domain,
            COUNT(*) as citation_count
        FROM citations
        WHERE domain = ANY(%s) AND created_at >= CURRENT_DATE - INTERVAL '%s days'
        GROUP BY DATE(created_at), domain
        ORDER BY date DESC, citation_count DESC
    """ % ("%s", days), (top_domains,))
    
    rows = cur.fetchall()
    print(tabulate(rows, headers=["æ—¥æœŸ", "åŸŸå", "å¼•ç”¨æ¬¡æ•°"], tablefmt="grid"))
    
    conn.close()

def analyze_platform_comparison():
    """5. å¹³å°å¯¹æ¯”åˆ†æï¼ˆæ–°å¢ï¼‰"""
    print_header("å¹³å°å¯¹æ¯”åˆ†æ - DeepSeek vs è±†åŒ…")
    
    conn, cur = get_db_cursor()
    
    cur.execute("""
        SELECT 
            platform as "å¹³å°",
            COUNT(DISTINCT keyword) as "ç›‘æ§å…³é”®è¯æ•°",
            COUNT(*) as "æ€»æœç´¢æ¬¡æ•°",
            ROUND(AVG(response_time_ms)/1000.0, 2) as "å¹³å‡å“åº”æ—¶é—´(ç§’)",
            SUM(CASE WHEN search_status = 'completed' THEN 1 ELSE 0 END) as "æˆåŠŸæ¬¡æ•°",
            SUM(CASE WHEN search_status = 'failed' THEN 1 ELSE 0 END) as "å¤±è´¥æ¬¡æ•°"
        FROM search_records
        GROUP BY platform
    """)
    
    rows = cur.fetchall()
    print(tabulate(rows, headers=["å¹³å°", "å…³é”®è¯æ•°", "æœç´¢æ¬¡æ•°", "å¹³å‡å“åº”(ç§’)", "æˆåŠŸ", "å¤±è´¥"], tablefmt="grid"))
    
    conn.close()

def analyze_response_performance():
    """6. å“åº”æ€§èƒ½åˆ†æï¼ˆæ–°å¢ï¼‰"""
    print_header("å“åº”æ€§èƒ½åˆ†æ - æœç´¢é€Ÿåº¦ç»Ÿè®¡")
    
    conn, cur = get_db_cursor()
    
    cur.execute("""
        SELECT 
            keyword as "å…³é”®è¯",
            platform as "å¹³å°",
            ROUND(response_time_ms/1000.0, 2) as "å“åº”æ—¶é—´(ç§’)",
            (SELECT COUNT(*) FROM citations WHERE record_id = search_records.id) as "å¼•ç”¨æ•°",
            (SELECT COUNT(*) FROM search_queries WHERE record_id = search_records.id) as "æ‹“å±•è¯æ•°",
            created_at as "æ‰§è¡Œæ—¶é—´"
        FROM search_records
        WHERE search_status = 'completed' AND response_time_ms IS NOT NULL
        ORDER BY created_at DESC
        LIMIT 10
    """)
    
    rows = cur.fetchall()
    print(tabulate(rows, headers=["å…³é”®è¯", "å¹³å°", "å“åº”æ—¶é—´(ç§’)", "å¼•ç”¨æ•°", "æ‹“å±•è¯æ•°", "æ‰§è¡Œæ—¶é—´"], tablefmt="grid"))
    
    conn.close()

def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "ğŸš€ "*20)
    print("    GEO æ·±åº¦æ´å¯ŸæŠ¥å‘Š (å¢å¼ºç‰ˆ v2.0)")
    print("    é›†æˆï¼šjiebaåˆ†è¯ | SoVåˆ†æ | æ—¶é—´è¶‹åŠ¿ | å¹³å°å¯¹æ¯”")
    print("ğŸš€ "*20)
    
    try:
        # 1. æ ¸å¿ƒä¿¡ä»»æºï¼ˆè¿‘ 7 å¤©ï¼‰
        analyze_trust_sources(days=7)
        
        # 2. AI æœç´¢æ„å›¾ï¼ˆjieba åˆ†è¯ï¼‰
        analyze_search_intent()
        
        # 3. å“ç‰Œæ›å…‰çŸ©é˜µ
        analyze_brand_exposure()
        
        # 4. æ—¶é—´è¶‹åŠ¿åˆ†æ
        analyze_time_trends(days=7)
        
        # 5. å¹³å°å¯¹æ¯”åˆ†æ
        analyze_platform_comparison()
        
        # 6. å“åº”æ€§èƒ½åˆ†æ
        analyze_response_performance()
        
        print("\n" + "="*80)
        print("âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼")
        print("="*80 + "\n")
        
    except Exception as e:
        print(f"\nâŒ è·å–ç»Ÿè®¡æ•°æ®å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
