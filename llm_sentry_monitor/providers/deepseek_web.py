import time
import os
import json
import re
from playwright.sync_api import sync_playwright
from providers.base import BaseProvider
from core.parser import extract_domain
from core.logger_config import setup_logger

class DeepSeekWebProvider(BaseProvider):
    def search(self, keyword: str, prompt: str):
        user_data_dir = os.path.join(os.getenv("BROWSER_DATA_DIR", "./browser_data"), "deepseek")
        
        # ç”¨äºå­˜å‚¨æ‹¦æˆªåˆ°çš„æœç´¢ç»“æœ
        captured_search_results = []
        captured_queries = []  # å­˜å‚¨ AI æ‹“å±•çš„æœç´¢è¯
        full_response_text = ""
        
        def handle_response(response):
            """æ‹¦æˆª API å“åº”ï¼Œæå–æœç´¢ç»“æœå’Œæ‹“å±•è¯"""
            nonlocal captured_search_results, captured_queries, full_response_text
            
            url_lower = response.url.lower()
            # æ‰©å±•APIç«¯ç‚¹åŒ¹é…æ¨¡å¼
            api_patterns = [
                "api/v0/chat/completion",
                "api/v1/chat/completion"
            ]
            self.logger.info(f"[ç½‘ç»œæ‹¦æˆª] å“åº”URL: {response.url}")
            if any(pattern in url_lower for pattern in api_patterns):
                matched_pattern = next((p for p in api_patterns if p in url_lower), "unknown")
                self.logger.info(f"[ç½‘ç»œæ‹¦æˆª] APIç«¯ç‚¹åŒ¹é…: {matched_pattern}")
                try:
                    content_type = response.headers.get("content-type", "")
                    
                    # å¤„ç† SSE æµ
                    if "text/event-stream" in content_type or "stream" in url_lower:
                        try:
                            body = response.text()
                            self.logger.info(f"[ç½‘ç»œæ‹¦æˆª] SSEæµå¼å“åº”ï¼Œå¼€å§‹è§£ææ•°æ®")
                            
                            # æ­£ç¡®è§£æ SSE æ•°æ®æµ
                            # SSE æ ¼å¼ï¼šäº‹ä»¶ä¹‹é—´ç”¨ç©ºè¡Œåˆ†éš”ï¼Œä¸€ä¸ªäº‹ä»¶å¯ä»¥æœ‰å¤šè¡Œ data:
                            events = []
                            current_event_data = []
                            
                            for line in body.split('\n'):
                                line = line.rstrip('\r')  # ç§»é™¤å¯èƒ½çš„ \r
                                
                                if line.startswith('data: '):
                                    # æ”¶é›†å¤šè¡Œ data: å­—æ®µ
                                    data_content = line[6:]  # å»æ‰ "data: " å‰ç¼€
                                    current_event_data.append(data_content)
                                elif line == '':
                                    # ç©ºè¡Œè¡¨ç¤ºäº‹ä»¶ç»“æŸï¼Œåˆå¹¶æ‰€æœ‰ data: è¡Œ
                                    if current_event_data:
                                        # å¤šè¡Œ data: åº”è¯¥ç”¨æ¢è¡Œç¬¦è¿æ¥
                                        combined_data = '\n'.join(current_event_data)
                                        events.append(combined_data)
                                        current_event_data = []
                                elif line.startswith('event:') or line.startswith('id:') or line.startswith('retry:'):
                                    # å¿½ç•¥å…¶ä»– SSE å­—æ®µï¼ˆevent, id, retryï¼‰
                                    continue
                            
                            # å¤„ç†æœ€åä¸€ä¸ªäº‹ä»¶ï¼ˆå¦‚æœæ²¡æœ‰ä»¥ç©ºè¡Œç»“å°¾ï¼‰
                            if current_event_data:
                                combined_data = '\n'.join(current_event_data)
                                events.append(combined_data)
                            
                            self.logger.debug(f"[SSEè§£æ] å…±è§£æåˆ° {len(events)} ä¸ª SSE äº‹ä»¶")
                            
                            # å¤„ç†æ¯ä¸ªäº‹ä»¶çš„æ•°æ®
                            for event_data in events:
                                try:
                                    json_str = event_data.strip()
                                    if json_str and json_str != '[DONE]' and json_str != 'null':
                                        data = json.loads(json_str)
                                        
                                        # æå–æœç´¢ç»“æœå’Œæ‹“å±•è¯
                                        if 'v' in data:
                                            # æƒ…å†µ1: å®Œæ•´çš„ fragments æ•°æ®
                                            if isinstance(data['v'], dict):
                                                response_data = data['v'].get('response', {})
                                                fragments = response_data.get('fragments', [])
                                                for frag in fragments:
                                                    if frag.get('type') == 'SEARCH':
                                                        # æå–æ‹“å±•è¯ (queries)
                                                        queries = frag.get('queries', [])
                                                        queries_before = len(captured_queries)
                                                        for q in queries:
                                                            if isinstance(q, dict):
                                                                query_text = q.get('query', q.get('text', ''))
                                                            else:
                                                                query_text = str(q)
                                                            if query_text and query_text not in captured_queries:
                                                                captured_queries.append(query_text)
                                                                self.logger.info(f"[æ•°æ®æŠ“å–] æŸ¥è¯¢è¯: {query_text}")
                                                        
                                                        if len(captured_queries) > queries_before:
                                                            self.logger.info(f"[æ•°æ®æŠ“å–] è¿›åº¦: {len(captured_queries)} ä¸ªæŸ¥è¯¢, {len(captured_search_results)} ä¸ªç½‘ç«™")
                                                        
                                                        # æå–æœç´¢ç»“æœ (results)
                                                        results = frag.get('results', [])
                                                        results_before = len(captured_search_results)
                                                        for r in results:
                                                            if isinstance(r, dict) and r.get('url'):
                                                                url = r.get('url', '')
                                                                domain = extract_domain(url)
                                                                captured_search_results.append({
                                                                    "url": url,
                                                                    "title": r.get('title', r.get('name', '')),
                                                                    "snippet": r.get('snippet', r.get('description', '')),
                                                                    "site_name": r.get('site_name', r.get('source', '')),
                                                                    "cite_index": r.get('cite_index', r.get('index', 0))
                                                                })
                                                                self.logger.info(f"[æ•°æ®æŠ“å–] ç½‘ç«™: {url[:60]}... (åŸŸå: {domain})")
                                                        
                                                        if len(captured_search_results) > results_before:
                                                            self.logger.info(f"[æ•°æ®æŠ“å–] è¿›åº¦: {len(captured_queries)} ä¸ªæŸ¥è¯¢, {len(captured_search_results)} ä¸ªç½‘ç«™")
                                            
                                            # æƒ…å†µ2: å¢é‡æ›´æ–°çš„ results æ•°ç»„ï¼ˆå…³é”®ä¿®å¤ï¼‰
                                            elif isinstance(data['v'], list):
                                                # æ£€æŸ¥è·¯å¾„å‚æ•°ï¼Œç¡®è®¤æ˜¯å¦æ˜¯ results æ›´æ–°
                                                path = data.get('p', '')
                                                
                                                # å¤„ç†å¢é‡æ›´æ–°çš„ results: {"p":"response/fragments/-1/results","v":[...]}
                                                if 'results' in path.lower() or (len(data['v']) > 0 and isinstance(data['v'][0], dict) and 'url' in data['v'][0]):
                                                    results_before = len(captured_search_results)
                                                    for r in data['v']:
                                                        if isinstance(r, dict) and r.get('url'):
                                                            url = r.get('url', '')
                                                            domain = extract_domain(url)
                                                            captured_search_results.append({
                                                                "url": url,
                                                                "title": r.get('title', r.get('name', '')),
                                                                "snippet": r.get('snippet', r.get('description', '')),
                                                                "site_name": r.get('site_name', r.get('source', '')),
                                                                "cite_index": r.get('cite_index', r.get('index', 0))
                                                            })
                                                            self.logger.info(f"ä» API å¢é‡æ›´æ–°æ•è·ç½‘ç«™: {url[:60]}... (åŸŸå: {domain}, cite_index: {r.get('cite_index', 0)})")
                                                    
                                                    if len(captured_search_results) > results_before:
                                                        self.logger.info(f"å½“å‰å·²æ•è·: {len(captured_queries)} ä¸ªæŸ¥è¯¢, {len(captured_search_results)} ä¸ªç½‘ç«™")
                                                
                                                # å¤„ç†å¢é‡æ›´æ–°çš„ queries: {"p":"response/fragments/-1/queries","v":[...]}
                                                elif 'queries' in path.lower() or (len(data['v']) > 0 and not isinstance(data['v'][0], dict)):
                                                    queries_before = len(captured_queries)
                                                    for q in data['v']:
                                                        if isinstance(q, dict):
                                                            query_text = q.get('query', q.get('text', ''))
                                                        else:
                                                            query_text = str(q)
                                                        if query_text and query_text not in captured_queries:
                                                            captured_queries.append(query_text)
                                                            self.logger.info(f"ä» API å¢é‡æ›´æ–°æ•è·æŸ¥è¯¢: \"{query_text}\"")
                                                    
                                                    if len(captured_queries) > queries_before:
                                                        self.logger.info(f"å½“å‰å·²æ•è·: {len(captured_queries)} ä¸ªæŸ¥è¯¢, {len(captured_search_results)} ä¸ªç½‘ç«™")
                                        
                                        # å°è¯•å…¶ä»–å¯èƒ½çš„æ•°æ®ç»“æ„
                                        # ç›´æ¥åŒ…å« results æˆ– queries
                                        if 'results' in data and isinstance(data['results'], list):
                                            results_before = len(captured_search_results)
                                            for r in data['results']:
                                                if isinstance(r, dict) and r.get('url'):
                                                    url = r.get('url', '')
                                                    domain = extract_domain(url)
                                                    captured_search_results.append({
                                                        "url": url,
                                                        "title": r.get('title', r.get('name', '')),
                                                        "snippet": r.get('snippet', r.get('description', '')),
                                                        "site_name": r.get('site_name', r.get('source', '')),
                                                        "cite_index": r.get('cite_index', r.get('index', 0))
                                                    })
                                                    self.logger.info(f"ä» SSE (resultså­—æ®µ) æå–åˆ°ç½‘ç«™: {url[:60]}... (åŸŸå: {domain})")
                                            
                                            if len(captured_search_results) > results_before:
                                                self.logger.info(f"å½“å‰å·²æ•è·: {len(captured_queries)} ä¸ªæŸ¥è¯¢, {len(captured_search_results)} ä¸ªç½‘ç«™")
                                        
                                        if 'queries' in data and isinstance(data['queries'], list):
                                            queries_before = len(captured_queries)
                                            for q in data['queries']:
                                                if isinstance(q, dict):
                                                    query_text = q.get('query', q.get('text', ''))
                                                else:
                                                    query_text = str(q)
                                                if query_text and query_text not in captured_queries:
                                                    captured_queries.append(query_text)
                                                    self.logger.info(f"ä» SSE (querieså­—æ®µ) æå–åˆ°æŸ¥è¯¢: \"{query_text}\"")
                                            
                                            if len(captured_queries) > queries_before:
                                                self.logger.info(f"å½“å‰å·²æ•è·: {len(captured_queries)} ä¸ªæŸ¥è¯¢, {len(captured_search_results)} ä¸ªç½‘ç«™")
                                        
                                        # æå–å›ç­”å†…å®¹
                                        if 'content' in data:
                                            content = data.get('content', '')
                                            if isinstance(content, str) and content:
                                                full_response_text += content
                                        elif 'delta' in data and 'content' in data.get('delta', {}):
                                            content = data['delta'].get('content', '')
                                            if isinstance(content, str) and content:
                                                full_response_text += content
                                                
                                except json.JSONDecodeError as e:
                                    self.logger.debug(f"JSON è§£æå¤±è´¥: {e}")
                                    continue
                        except Exception as e:
                            self.logger.debug(f"è§£æ SSE å“åº”å¤±è´¥: {e}")
                    
                    # å¤„ç†æ™®é€š JSON å“åº”
                    elif "application/json" in content_type:
                        try:
                            data = response.json()
                            self.logger.debug(f"æ‹¦æˆªåˆ° JSON å“åº”: {response.url[:100]}")
                            
                            # æå–æœç´¢ç›¸å…³ä¿¡æ¯
                            if 'search' in data:
                                search_data = data['search']
                                if 'queries' in search_data:
                                    queries = search_data['queries']
                                    queries_before = len(captured_queries)
                                    if isinstance(queries, list):
                                        for q in queries:
                                            query_text = q if isinstance(q, str) else q.get('query', '')
                                            if query_text and query_text not in captured_queries:
                                                captured_queries.append(query_text)
                                                self.logger.info(f"ä» JSON å“åº”æå–åˆ°æŸ¥è¯¢: \"{query_text}\"")
                                    
                                    if len(captured_queries) > queries_before:
                                        self.logger.info(f"å½“å‰å·²æ•è·: {len(captured_queries)} ä¸ªæŸ¥è¯¢, {len(captured_search_results)} ä¸ªç½‘ç«™")
                                
                                if 'results' in search_data:
                                    results_before = len(captured_search_results)
                                    for r in search_data['results']:
                                        if isinstance(r, dict) and r.get('url'):
                                            url = r.get('url', '')
                                            domain = extract_domain(url)
                                            captured_search_results.append({
                                                "url": url,
                                                "title": r.get('title', ''),
                                                "snippet": r.get('snippet', ''),
                                                "site_name": r.get('site_name', r.get('source', '')),
                                                "cite_index": r.get('cite_index', r.get('index', 0))
                                            })
                                            self.logger.info(f"ä» JSON å“åº”æå–åˆ°ç½‘ç«™: {url[:60]}... (åŸŸå: {domain})")
                                    
                                    if len(captured_search_results) > results_before:
                                        self.logger.info(f"å½“å‰å·²æ•è·: {len(captured_queries)} ä¸ªæŸ¥è¯¢, {len(captured_search_results)} ä¸ªç½‘ç«™")
                        except Exception as e:
                            self.logger.debug(f"è§£æ JSON å“åº”å¤±è´¥: {e}")
                            
                except Exception as e:
                    self.logger.debug(f"æ‹¦æˆªå“åº”å¤±è´¥: {e}")
        
        with sync_playwright() as p:
            browser = p.chromium.launch_persistent_context(
                user_data_dir=user_data_dir,
                headless=self.headless,
                args=["--disable-blink-features=AutomationControlled"]
            )
            
            try:
                page = browser.pages[0] if browser.pages else browser.new_page()
                page.set_default_timeout(self.timeout)
                
                # æ³¨å†Œå“åº”æ‹¦æˆªå™¨
                page.on("response", handle_response)
                
                self.logger.info("æ­£åœ¨æ‰“å¼€ DeepSeek é¦–é¡µ...")
                page.goto("https://chat.deepseek.com/")
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦ç™»å½•
                time.sleep(2)
                if "login" in page.url or page.query_selector("text=ç™»å½•"):
                    self.logger.warning("æ£€æµ‹åˆ°å¯èƒ½éœ€è¦ç™»å½•ï¼Œè¯·åœ¨æµè§ˆå™¨çª—å£ä¸­å®Œæˆç™»å½•...")
                    try:
                        page.wait_for_url("**/chat.deepseek.com/**", timeout=120000)
                    except:
                        self.logger.error("ç™»å½•è¶…æ—¶ï¼Œè¯·ç¡®ä¿å·²æ‰‹åŠ¨ç™»å½•å¹¶ä¿å­˜çŠ¶æ€ã€‚")
                
                # 1. ç­‰å¾…è¾“å…¥æ¡†åŠ è½½å¹¶è¾“å…¥
                page.wait_for_selector("textarea", timeout=self.timeout)
                page.click("textarea")
                time.sleep(0.5)
                page.fill("textarea", prompt)
                self.logger.info(f"å·²è¾“å…¥æé—®: {prompt[:50]}...")
                time.sleep(1)
                
                # 2. å¼€å¯"è”ç½‘æœç´¢" - æ™ºèƒ½åˆ¤æ–­çŠ¶æ€
                try:
                    # å¯»æ‰¾åŒ…å«â€œè”ç½‘æœç´¢â€æ–‡å­—çš„æŒ‰é’®å®¹å™¨
                    # DeepSeek çš„å¼€å…³é€šå¸¸æ˜¯ä¸€ä¸ªåŒ…å«å›¾æ ‡å’Œæ–‡å­—çš„ div
                    search_toggle = page.locator("div:has-text('è”ç½‘æœç´¢')").last
                    
                    if search_toggle.is_visible():
                        # æ£€æŸ¥æ˜¯å¦å·²ç»æ¿€æ´»
                        # é€»è¾‘ï¼šæ£€æŸ¥è¯¥å…ƒç´ æˆ–å…¶çˆ¶çº§æ˜¯å¦åŒ…å«ç‰¹å®šçš„æ¿€æ´»ç±»åï¼ˆå¦‚ 'checked' æˆ–é¢œè‰²ç›¸å…³çš„ç±»ï¼‰
                        # æˆ–è€…æ£€æŸ¥å…¶å†…éƒ¨æ˜¯å¦æœ‰ç‰¹å®šçš„æ¿€æ´»æ ·å¼
                        is_active = False
                        
                        # æ–¹æ¡ˆ A: æ£€æŸ¥ class ä¸­æ˜¯å¦åŒ…å« 'checked' æˆ– 'active' (DeepSeek å¸¸ç”¨)
                        class_attr = search_toggle.get_attribute("class") or ""
                        parent_class = ""
                        try:
                            parent_class = page.evaluate("el => el.parentElement.className", search_toggle.element_handle())
                        except: pass
                        
                        if "checked" in class_attr.lower() or "active" in class_attr.lower() or "checked" in str(parent_class).lower():
                            is_active = True
                        
                        # æ–¹æ¡ˆ B: æ£€æŸ¥é¢œè‰²ï¼ˆæ¿€æ´»æ—¶é€šå¸¸æ˜¯è“è‰² #247fff æˆ–ç±»ä¼¼ï¼‰
                        if not is_active:
                            color = page.evaluate("el => window.getComputedStyle(el).color", search_toggle.element_handle())
                            # å¦‚æœé¢œè‰²ä¸æ˜¯é»˜è®¤çš„ç°è‰²ï¼ˆæ¯”å¦‚å˜æˆäº†è“è‰²ï¼‰ï¼Œåˆ™è®¤ä¸ºå·²æ¿€æ´»
                            if "rgb(36, 127, 255)" in color or "rgb(0, 0, 0)" not in color: # ç®€å•åˆ¤æ–­éé»‘å³æ´»
                                is_active = True

                        if is_active:
                            self.logger.info("æ£€æµ‹åˆ°â€˜è”ç½‘æœç´¢â€™å·²é»˜è®¤å¼€å¯ï¼Œè·³è¿‡ç‚¹å‡»ã€‚")
                        else:
                            search_toggle.click()
                            self.logger.info("å·²æ‰‹åŠ¨å¼€å¯â€˜è”ç½‘æœç´¢â€™")
                            time.sleep(0.5)
                except Exception as e:
                    self.logger.warning(f"åˆ¤æ–­è”ç½‘æœç´¢çŠ¶æ€å¤±è´¥: {e}")
                
                # 3. ç‚¹å‡»å‘é€æŒ‰é’®
                try:
                    # æ ¹æ®æœ€æ–° UIï¼Œå‘é€æŒ‰é’®æ˜¯ä¸€ä¸ªè“è‰²çš„åœ†å½¢å›¾æ ‡æŒ‰é’®
                    # å°è¯•å¤šä¸ªå¯èƒ½çš„é€‰æ‹©å™¨
                    send_selectors = [
                        "div[class*='_7436f3']", # å¸¸è§çš„ç±»åæ¨¡å¼
                        "button:has(svg)",       # åŒ…å«å›¾æ ‡çš„æŒ‰é’®
                        ".ds-icon--send",        # å›¾æ ‡ç±»å
                        "div[role='button'] >> svg" # è§’è‰²ä¸ºæŒ‰é’®çš„ div ä¸‹çš„ svg
                    ]
                    
                    sent = False
                    for selector in send_selectors:
                        btn = page.locator(selector).last
                        if btn.is_visible() and btn.is_enabled():
                            btn.click()
                            sent = True
                            self.logger.info(f"é€šè¿‡é€‰æ‹©å™¨ {selector} ç‚¹å‡»äº†å‘é€æŒ‰é’®")
                            break
                    
                    if not sent:
                        # å¤‡é€‰ï¼šä½¿ç”¨é”®ç›˜å¿«æ·é”®
                        page.keyboard.press("Enter") # æˆ–è€… Control+Enter
                        self.logger.info("å·²é€šè¿‡ Enter é”®å‘é€")
                        
                except Exception as e:
                    self.logger.warning(f"ç‚¹å‡»å‘é€æŒ‰é’®å¤±è´¥: {e}")
                    page.keyboard.press("Control+Enter")
                
                self.logger.info("å·²å‘é€æé—®ï¼Œç­‰å¾… AI å›ç­”...")
                
                # 4. ç­‰å¾…å›ç­”ç”Ÿæˆå®Œæˆ
                time.sleep(5)  # ç­‰å¾…è¯·æ±‚å‘é€
                
                # ç­‰å¾…å›ç­”å®¹å™¨å‡ºç°
                content_selector = ".ds-markdown"
                try:
                    page.wait_for_selector(content_selector, timeout=self.timeout)
                except:
                    self.logger.warning("æœªå‘ç° .ds-markdown å®¹å™¨")
                
                # å¾ªç¯æ£€æŸ¥ç”ŸæˆçŠ¶æ€
                max_retries = 30
                last_content = ""
                for i in range(max_retries):
                    time.sleep(2)
                    try:
                        # å°è¯•è·å–å½“å‰å†…å®¹
                        content_el = page.query_selector(content_selector)
                        if content_el:
                            current_content = content_el.inner_text()
                            
                            # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆå®Œæˆ
                            if len(current_content) > 100:
                                if current_content == last_content:
                                    # å†…å®¹ä¸å†å˜åŒ–ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰"åœæ­¢ç”Ÿæˆ"æŒ‰é’®
                                    stop_btn = page.query_selector("text=åœæ­¢ç”Ÿæˆ")
                                    if not stop_btn:
                                        self.logger.info("å›ç­”ç”Ÿæˆå·²å®Œæˆ")
                                        full_response_text = current_content
                                        break
                                
                            last_content = current_content
                            self.logger.info(f"æ­£åœ¨ç”Ÿæˆä¸­... (å½“å‰é•¿åº¦: {len(current_content)}, å·²æ•è· {len(captured_search_results)} ä¸ªæœç´¢ç»“æœ)")
                    except Exception as e:
                        continue
                
                # 5. æ•°æ®å·²ä»ç½‘ç»œæ¥å£æŠ“å–å®Œæˆï¼Œä¼˜å…ˆä½¿ç”¨æ¥å£æ•°æ®
                if len(captured_search_results) == 0:
                    self.logger.warning("æœªé€šè¿‡ API æ¥å£æŠ“å–åˆ°å¼•ç”¨ï¼Œå°è¯•ä» DOM æå–ä½œä¸ºè¡¥å……...")
                    api_captured_urls = set()
                else:
                    self.logger.info(f"å·²é€šè¿‡ API æ¥å£æŠ“å–åˆ° {len(captured_search_results)} ä¸ªå¼•ç”¨")
                    api_captured_urls = {r.get('url', '') for r in captured_search_results if r.get('url')}
                
                # å¦‚æœæ¥å£æ²¡æœ‰æŠ“å–åˆ°æ•°æ®ï¼Œå°è¯•ä» DOM æå–ä½œä¸ºæœ€åæ‰‹æ®µ
                if len(captured_search_results) == 0:
                    try:
                        # å°è¯•å¤šç§æ–¹å¼æå–å¼•ç”¨é“¾æ¥
                        # DeepSeek ä½¿ç”¨ ds-markdown-cite ç±»æ ‡è®°å¼•ç”¨
                        # ä¼˜å…ˆæå–å¸¦å¼•ç”¨æ ‡è®°çš„é“¾æ¥
                        link_selectors = [
                            ".ds-markdown a[href^='http'] .ds-markdown-cite",  # ä¼˜å…ˆï¼šå¸¦å¼•ç”¨æ ‡è®°çš„é“¾æ¥
                            ".ds-markdown a[href^='https'] .ds-markdown-cite",
                            ".ds-markdown a[href^='http']",  # markdown å†…å®¹ä¸­çš„æ‰€æœ‰é“¾æ¥
                            ".ds-markdown a[href^='https']",
                            "a[href^='http'] .ds-markdown-cite",  # æ‰€æœ‰å¸¦å¼•ç”¨æ ‡è®°çš„é“¾æ¥
                            "a[href^='https'] .ds-markdown-cite",
                            "a[href^='http']",  # æ‰€æœ‰å¤–éƒ¨é“¾æ¥
                            "a[href^='https']",
                            "[class*='citation'] a",  # å¼•ç”¨ç›¸å…³çš„é“¾æ¥
                            "[class*='reference'] a",
                            "[class*='source'] a",  # æ¥æºç›¸å…³çš„é“¾æ¥
                        ]
                        
                        seen_dom_urls = set(api_captured_urls)  # ä» API å·²æ•è·çš„ URL å¼€å§‹
                        dom_extracted_count = 0
                        
                        for selector in link_selectors:
                            try:
                                links = page.query_selector_all(selector)
                                self.logger.debug(f"é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(links)} ä¸ªé“¾æ¥")
                                
                                for link in links:
                                    try:
                                        # å¦‚æœé€‰æ‹©å™¨åŒ¹é…çš„æ˜¯ .ds-markdown-citeï¼Œéœ€è¦æ‰¾åˆ°çˆ¶é“¾æ¥
                                        link_tag = link.evaluate("el => el.tagName.toLowerCase()")
                                        link_class = link.get_attribute("class") or ""
                                        
                                        if link_tag == 'span' or 'ds-markdown-cite' in link_class:
                                            # æ‰¾åˆ°çˆ¶çº§ a æ ‡ç­¾
                                            try:
                                                parent_a = link.evaluate_handle("el => el.closest('a')")
                                                if parent_a:
                                                    link = parent_a
                                                else:
                                                    # å¦‚æœæ‰¾ä¸åˆ°çˆ¶ aï¼Œè·³è¿‡è¿™ä¸ªå…ƒç´ 
                                                    continue
                                            except:
                                                continue
                                        
                                        href = link.get_attribute("href")
                                        if not href:
                                            continue
                                        
                                        # è¿‡æ»¤æ‰ DeepSeek è‡ªå·±çš„åŸŸå
                                        if any(d in href.lower() for d in ["deepseek.com", "deepseek.ai"]):
                                            continue
                                        
                                        # å»é‡
                                        if href in seen_dom_urls:
                                            continue
                                        seen_dom_urls.add(href)
                                        
                                        # æå–å¼•ç”¨åºå·ï¼ˆå…³é”®ä¿®å¤ï¼šä» ds-markdown-cite ä¸­æå–ï¼‰
                                        cite_index = 0
                                        try:
                                            # æŸ¥æ‰¾é“¾æ¥å†…çš„ ds-markdown-cite å…ƒç´ 
                                            cite_element = link.query_selector(".ds-markdown-cite")
                                            if cite_element:
                                                # ä» cite å…ƒç´ ä¸­æå–åºå·
                                                cite_text = cite_element.inner_text().strip()
                                                # å°è¯•ä»æ–‡æœ¬ä¸­æå–æ•°å­—ï¼ˆå¦‚ "1", "2"ï¼‰
                                                import re
                                                match = re.search(r'\d+', cite_text)
                                                if match:
                                                    cite_index = int(match.group())
                                                else:
                                                    # å°è¯•ä» span çš„ç»å¯¹å®šä½å…ƒç´ ä¸­æå–
                                                    cite_number = cite_element.evaluate("""
                                                        el => {
                                                            let spans = el.querySelectorAll('span');
                                                            for (let span of spans) {
                                                                let text = span.textContent.trim();
                                                                let num = parseInt(text);
                                                                if (!isNaN(num) && num > 0) {
                                                                    return num;
                                                                }
                                                            }
                                                            return 0;
                                                        }
                                                    """)
                                                    cite_index = cite_number or 0
                                        except Exception as e:
                                            self.logger.debug(f"æå–å¼•ç”¨åºå·å¤±è´¥: {e}")
                                        
                                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åºå·ï¼Œå°è¯•ä»é“¾æ¥å‘¨å›´çš„æ–‡æœ¬ä¸­æå–
                                        if cite_index == 0:
                                            try:
                                                # æŸ¥æ‰¾é“¾æ¥å‰çš„å¼•ç”¨æ ‡è®°
                                                prev_text = link.evaluate("""
                                                    el => {
                                                        let text = el.textContent || '';
                                                        let match = text.match(/\\[(\\d+)\\]/);
                                                        if (match) return parseInt(match[1]);
                                                        
                                                        // æŸ¥æ‰¾çˆ¶å…ƒç´ ä¸­çš„å¼•ç”¨æ ‡è®°
                                                        let parent = el.parentElement;
                                                        if (parent) {
                                                            let parentText = parent.textContent || '';
                                                            let parentMatch = parentText.match(/\\[(\\d+)\\]/);
                                                            if (parentMatch) return parseInt(parentMatch[1]);
                                                        }
                                                        return 0;
                                                    }
                                                """)
                                                cite_index = prev_text or 0
                                            except:
                                                pass
                                        
                                        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°åºå·ï¼Œä½¿ç”¨å½“å‰è®¡æ•°
                                        if cite_index == 0:
                                            cite_index = len(captured_search_results) + 1
                                        
                                        # æå–æ ‡é¢˜
                                        title = link.inner_text().strip()
                                        # ç§»é™¤å¼•ç”¨æ ‡è®°ï¼ˆå¦‚ [1]ï¼‰ä»æ ‡é¢˜ä¸­
                                        import re
                                        title = re.sub(r'\[\d+\]', '', title).strip()
                                        
                                        if not title:
                                            # å°è¯•ä»çˆ¶å…ƒç´ æˆ–é™„è¿‘å…ƒç´ è·å–
                                            try:
                                                parent_text = link.evaluate("""
                                                    el => {
                                                        let parent = el.parentElement;
                                                        if (parent) {
                                                            let text = parent.textContent || '';
                                                            // ç§»é™¤å¼•ç”¨æ ‡è®°
                                                            text = text.replace(/\\[\\d+\\]/g, '').trim();
                                                            return text.substring(0, 100);
                                                        }
                                                        return '';
                                                    }
                                                """)
                                                title = parent_text
                                            except:
                                                pass
                                        
                                        # æå–æ‘˜è¦ï¼ˆå°è¯•ä»é™„è¿‘å…ƒç´ ï¼‰
                                        snippet = ""
                                        try:
                                            sibling_text = link.evaluate("""
                                                el => {
                                                    let next = el.nextElementSibling;
                                                    if (next && next.textContent) {
                                                        return next.textContent.trim().substring(0, 200);
                                                    }
                                                    let parent = el.parentElement;
                                                    if (parent && parent.nextElementSibling) {
                                                        return parent.nextElementSibling.textContent.trim().substring(0, 200);
                                                    }
                                                    return '';
                                                }
                                            """)
                                            snippet = sibling_text
                                        except:
                                            pass
                                        
                                        captured_search_results.append({
                                            "url": href,
                                            "title": title or extract_domain(href),
                                            "snippet": snippet,
                                            "site_name": extract_domain(href),
                                            "cite_index": cite_index
                                        })
                                        dom_extracted_count += 1
                                        self.logger.debug(f"ä» DOM æ•è·å¼•ç”¨: {href[:50]}... (cite_index: {cite_index})")
                                    except Exception as e:
                                        self.logger.debug(f"æå–é“¾æ¥å¤±è´¥: {e}")
                                        continue
                            except Exception as e:
                                self.logger.debug(f"é€‰æ‹©å™¨ '{selector}' æ‰§è¡Œå¤±è´¥: {e}")
                                continue
                    
                        self.logger.info(f"ä» DOM æå–åˆ° {dom_extracted_count} ä¸ªæ–°å¼•ç”¨é“¾æ¥ï¼ˆAPI å·²æ•è· {len(api_captured_urls)} ä¸ªï¼‰")
                        
                        # å°è¯•æŸ¥æ‰¾å¼•ç”¨åˆ—è¡¨åŒºåŸŸï¼ˆDeepSeek å¯èƒ½åœ¨åº•éƒ¨æˆ–ä¾§è¾¹æ˜¾ç¤ºå¼•ç”¨åˆ—è¡¨ï¼‰
                        try:
                            # æŸ¥æ‰¾å¯èƒ½çš„å¼•ç”¨åˆ—è¡¨å®¹å™¨
                            citation_containers = [
                                "[class*='citation']",
                                "[class*='reference']",
                                "[class*='source']",
                                "[class*='link-list']",
                                "[class*='reference-list']"
                            ]
                            
                            for container_selector in citation_containers:
                                try:
                                    containers = page.query_selector_all(container_selector)
                                    if containers:
                                        self.logger.debug(f"æ‰¾åˆ° {len(containers)} ä¸ªå¯èƒ½çš„å¼•ç”¨å®¹å™¨: {container_selector}")
                                        for container in containers:
                                            # åœ¨å®¹å™¨å†…æŸ¥æ‰¾é“¾æ¥
                                            container_links = container.query_selector_all("a[href^='http']")
                                            for link in container_links:
                                                try:
                                                    href = link.get_attribute("href")
                                                    if href and href not in seen_dom_urls:
                                                        seen_dom_urls.add(href)
                                                        title = link.inner_text().strip() or extract_domain(href)
                                                        captured_search_results.append({
                                                            "url": href,
                                                            "title": title,
                                                            "snippet": "",
                                                            "site_name": extract_domain(href),
                                                            "cite_index": len(captured_search_results) + 1
                                                        })
                                                        dom_extracted_count += 1
                                                except:
                                                    continue
                                except:
                                    continue
                        except Exception as e:
                            self.logger.debug(f"æŸ¥æ‰¾å¼•ç”¨åˆ—è¡¨å®¹å™¨å¤±è´¥: {e}")
                    except Exception as e:
                        self.logger.warning(f"ä» DOM æå–å¼•ç”¨å¤±è´¥: {e}")
                
                # 6. æ•´ç†æœç´¢ç»“æœï¼ˆå»é‡ï¼‰
                seen_urls = set()
                unique_citations = []
                for result in captured_search_results:
                    url = result.get('url', '')
                    if url and url not in seen_urls:
                        seen_urls.add(url)
                        unique_citations.append({
                            "url": url,
                            "title": result.get('title', ''),
                            "snippet": result.get('snippet', ''),
                            "site_name": result.get('site_name', ''),
                            "cite_index": result.get('cite_index', 0)
                        })
                
                # æŒ‰ cite_index æ’åº
                unique_citations.sort(key=lambda x: x.get('cite_index', 999))
                
                # è®¡ç®—æ•°æ®æ¥æºç»Ÿè®¡
                api_captured_count = len(api_captured_urls)
                dom_extracted_count = len(unique_citations) - api_captured_count
                if dom_extracted_count < 0:
                    dom_extracted_count = 0
                
                # æ•°æ®æ•è·æ±‡æ€»æ—¥å¿—
                self.logger.info("")
                self.logger.info("=" * 60)
                self.logger.info("ğŸ“Š æ•°æ®æ•è·æ±‡æ€»")
                self.logger.info("=" * 60)
                
                # æŸ¥è¯¢ä¿¡æ¯æ±‡æ€»
                self.logger.info(f"ğŸ” æŸ¥è¯¢ä¿¡æ¯ (å…± {len(captured_queries)} ä¸ª):")
                if captured_queries:
                    for idx, q in enumerate(captured_queries, 1):
                        self.logger.info(f"  {idx}. \"{q}\"")
                else:
                    self.logger.info("  (æœªæ•è·åˆ°æŸ¥è¯¢)")
                
                # ç½‘ç«™ä¿¡æ¯æ±‡æ€»
                self.logger.info("")
                self.logger.info(f"ğŸŒ æŠ“å–ç½‘ç«™ (å…± {len(unique_citations)} ä¸ªå”¯ä¸€ç½‘ç«™):")
                self.logger.info(f"  - API æ‹¦æˆª: {api_captured_count} ä¸ª")
                self.logger.info(f"  - DOM æå–: {dom_extracted_count} ä¸ª")
                
                if unique_citations:
                    # æŒ‰åŸŸååˆ†ç»„ç»Ÿè®¡
                    domain_count = {}
                    for cite in unique_citations:
                        domain = cite.get('site_name', 'unknown')
                        domain_count[domain] = domain_count.get(domain, 0) + 1
                    
                    self.logger.info("")
                    self.logger.info("  ç½‘ç«™åˆ—è¡¨ (å‰15ä¸ª):")
                    for cite in unique_citations[:15]:
                        cite_index = cite.get('cite_index', 0)
                        site_name = cite.get('site_name', 'unknown')
                        title = cite.get('title', '')[:40] or '(æ— æ ‡é¢˜)'
                        url = cite.get('url', '')[:50]
                        self.logger.info(f"    [{cite_index}] {site_name}: {title}... ({url}...)")
                    
                    if len(unique_citations) > 15:
                        self.logger.info(f"    ... è¿˜æœ‰ {len(unique_citations) - 15} ä¸ªç½‘ç«™æœªæ˜¾ç¤º")
                    
                    self.logger.info("")
                    self.logger.info("  åŸŸååˆ†å¸ƒ (å‰10ä¸ª):")
                    sorted_domains = sorted(domain_count.items(), key=lambda x: x[1], reverse=True)
                    for domain, count in sorted_domains[:10]:
                        self.logger.info(f"    {domain}: {count} æ¬¡")
                else:
                    self.logger.info("  (æœªæ•è·åˆ°ç½‘ç«™)")
                
                self.logger.info("")
                self.logger.info("=" * 60)
                self.logger.info("âœ… æ•°æ®æ•è·å®Œæˆ")
                self.logger.info(f"   - æŸ¥è¯¢: {len(captured_queries)} ä¸ª")
                self.logger.info(f"   - ç½‘ç«™: {len(unique_citations)} ä¸ª")
                self.logger.info("=" * 60)
                self.logger.info("")
                
                # å¦‚æœæ•è·æ•°é‡æ˜æ˜¾å°‘äºé¢„æœŸï¼Œè¾“å‡ºè°ƒè¯•ä¿¡æ¯
                if len(unique_citations) < 3:
                    self.logger.warning("âš ï¸ æ•è·åˆ°çš„å¼•ç”¨æ•°é‡è¾ƒå°‘ï¼Œå¯èƒ½å­˜åœ¨é—®é¢˜")
                    self.logger.info("ğŸ’¡ è°ƒè¯•å»ºè®®ï¼š")
                    self.logger.info("   1. æ£€æŸ¥é¡µé¢ä¸­æ˜¯å¦ç¡®å®æ˜¾ç¤ºäº†å¼•ç”¨é“¾æ¥")
                    self.logger.info("   2. æŸ¥çœ‹æµè§ˆå™¨å¼€å‘è€…å·¥å…·çš„ Network æ ‡ç­¾ï¼Œæ‰¾åˆ° API å“åº”")
                    self.logger.info("   3. æ£€æŸ¥é¡µé¢ HTML ä¸­å¼•ç”¨é“¾æ¥çš„å®é™…ç»“æ„")
                
                return {
                    "full_text": full_response_text or last_content,
                    "queries": captured_queries,  # æ‹“å±•è¯
                    "citations": unique_citations  # å‚è€ƒç½‘é¡µ
                }
            finally:
                browser.close()
